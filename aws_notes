AWS: S3 - Simple Storage Service.

ref: https://aws.amazon.com/es/documentation/s3/

INTRODUCTION:
============

you can put any data file in S3 (Simple Storage Service)
they are organiced in "Buckets".
-Bucket names are globally unique, so you have to take one with a name non taken yet.

easy to DNS CNAME a URL to your bucket.
bucket name: sergio-images
but my desired domein is sergio.collado.com 

so: sergio.collado.com -> collado-images.s3.amazonaws.com

and include in the web, as:

<html>
<body>
<img src="http://sergio.collado/path/to/logo" alt="AWS_lOGO">
...

ACCESS CONTROL:
==============

Objects, also have 4 types of access control:
 · IAM policies
 · Bucket policies
 · ACLs(Acess Control Lists) - AWS console
 · Query string autentification - gives expiring acess.
 
 
 more info about Buckets in: http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingBucketPolicies.html
 more info about ACLs in: http://docs.aws.amazon.com/AmazonS3/latest/dev/S3_ACLs_UsingACLs.html
 
 The bucket also saves some information about the files: type, creation date, ...
 more info in: http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingMetadata.html
 
STORING POLICIES:
=================

There are three ways to store policies:
 · Standard ( "11 9's") - that means that if you have 1 million files stored there, you will loose one, everty 10.000 years.
   · it comes at greatest cost, but is the default.
 · Reduced Redundancy Storage (RRS) - is ("4 9s") - it means that if you have 10000 files there, you'll probable loose one every year.
   · It cost is reduced about 20% / Great for reproducible assets
   · RRS, can trigger notifications, on "object missing". 
 · Glaciar it has ("11 9's), but is great draw back is queued retrieval -you cannot get it back in real time- 
   · But is cost is about the 10% of the standard option - which make a great choise for arcivals, and back-ups.
   
PRICING:
========

Starts around 0.10$ per GB/month for "standard" S3 - the fee gets cheaper, if you use more.

Bandwith:
  IN: FREE
  OUT: Same Region: free
       Out of region: Starts at 0.12$ GB/month.
  REQUEST al also costy: (PUT, GET, POST, LIST) - Starts at 0.0055/1000 request.
   
 Just have in mind, that data trasnsfer inside your region is free!  
   
   
ADVANCED FEATURES OF S3
=======================

 S3 also allows for:
 
  ·light, static, website hosting.
  ·Object squeduled expiration.
  ·Encryption Options
  ·Versioning
  ·Logging
  ·"Direct from browser" upload to s3
  ·Multipart files upload
  ·Requested pays
  ·BitTorrent Support
   
- Simple static website hosting: You can rename to a domain you own  (Easy CNAMEd).
- Index, and Error documents support.
- key prefix routing rules.
- Oject expiration squeduling -live cicle management- 
- Encription options (SSE, SSL, or your own) [SSE: Server Side Encription]
- Versioning. Once is On, it can never can go back to Off, so you will be storing all your development. ##
- Logging: you can get IP, operations, dates, . Can be On/Off anytime. Watch out! store the logs, in another Bucket!
- Direct on brownser upload: You can grant users to directly upload to your bucket uses IAM/STS services to generate a url for
the bucket object. 5GB limitation on file size. ref -> http://aws.amazon.com/articles/1434
 If you want to upload more than 5GB, you have to use multi-part files upload, that is just limited to SDK(Java, iOs,PHP, Ruby); 
 the client chuncks the file into pieces. Loads each piece separately. Client closes file. S3 reassembles the file. This also allows
 stop/star partial uploads. With this method the max file size is 5TB.
 ref: http://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html
 - Request payment. always if the client is a AWS registered partner/customer.
 - Bittorret support (P2P protocoll) max file size 5GB
 ref: http://docs.aws.amazon.com/AmazonS3/latest/dev/S3Torrent.html
 
 
 ·First steps: 
 
The best is to migrate all static content to S3 (images, CSS, Javascript, HTML, etc...)- this reliebes the load on your
EBS, so it can do more important tasks, with this, the cost of the EBS, is much more lower, and gets more performance.
   
The second step, is set up Glacier for archival, Control access, set up Livecicle policies, Investigate advanced users: 
like Website hosting, direct to S3 uploads.
 



log in main server server:

|___|  __|_  )
|_|   (     /
|___| ___||___|

<< sudo su -
cd /var/local/src

Gon ; s3tools.org/download
333
--íntasll from the commmand, get the link

 wget http:.... (he link copied proivied)
 
 >> mv download s3cmd.trg
 
  tax -xcvzf s3cmd.tgz
  
  cd s3cmd-1.5.0-alphay/
  
  Less install
  
  easteisnt way to install is : python .py instals
 
  s3cmd-1.5.0-alpha1
  
  
  we have provide the kees> using > credential needed, > access credenctiasn "Aceess Keys´2"
  
 
  
  intall the keys. 
  
  w3cdmd --help 
  s3cmd [feature] --help 
  
  
INTRODUCTION TO CLOUD FRONT:
============================

-> Cloud Front is a CDN - Content Delivery Network.
   It uses locations close to end user (Edges).
   Other populars CDNs are Akamai, Level 3.

DESCRIPTION:
===========

- Resources of static website assests: Images, CSS, JS, HTML, JSON.
- Distributions: products, documents, data.
- Streaming: audio, video.
    Streaming uses propietary protocols, unlike http. Cloud Front uses Adobe/macromedia RTMP (Real Time Messaging Protocol), also the is
    an encrypted version: RTMPE (Real Time Messaging Protocol Encrypted)
    
 This is natively integrated in AWS, you can specify S3 Buckets as origins. Its interface is very easy, also supports standard http/s
 also streaming with RTMP/E, and is very cost effective: Contents are only cached, at edges, when there is a user request, and you can 
 free that chache, on demand or on squedule.
 
  Data can be "expired":
     - On squedule
     - On demand.
 
 HOW TO SET A CLOUDFRONT:
 ========================
 
 First you have to define a "Distribution":
    - you have to specify if this will be HTTP, or Streaming.
    - Set an 'origin': any web-accesible web server will do: private server, EC2 or S3.
    - You can also specify: 
              - TTL (Time To Live)- this defines how often Cloud Front is going to check for a new version of the file.
              - Path matches - this specifies what parts of your origin web server are going to be cached.
              - Logging - allows log request to Cloud Front.
              
 At this time you have created a distribution domain, you can easily rename using CNAMEd "vanity URLs" - this is useful in cases as
 Websites (CSS, JS, images references) or Applications (media players, web, mobile, or desktop apps).
 
 FINAL USER ACCESS:
 ==================
 
 When a final user access to a CDN (Content Delivery Network) URL, CloudFront directs the user to the closes edje, so it serves
 the data faster.
 This is a pulling mechanism, so if the closes edje to a client, doesn't have the required data, that edje request the data to the
 the origin edge, and pull that data down local, so next time a client request the data in that location, it will already stored 
 in the closest edje, and serviced faster.
 
  
 
 INTRODUCTION TO ELASTIC CACHE:
===============================

- Elastic Cache is an in memory caching mechanism.
- Is 100% Under the hood.
  - 100% API compliant: User get, set, incr, decr, stats, the same way you do in mem cache.
- More properly is mem cache cluster.


 what is memcache/d ?
 --------------------
 
 is a very popular, open source in-memory chache. very fast reads and writes. Because results came from memory not form disk.
 takes the burden from the back end resources and free them for more important work.
 This can be:
    - Databases.
    - Disk heads.
    - CPUs.
    
 The extra 'd' stands for deamon.
 
 Anithing that is expensive to create and/or nood be in real time. as:
    - Database query queues.
    - Large disk of files of the disk.
    - Computational expensive operations.
 Or if you want sombething to be extremely fast, as:
    - User sessions.
    - Product catalogs.
    
· Memcache is actually a key-value store.
Actually NoSQL is a memcache database.
- The access method is a key - which returns the value. It has no "query" capabilites other than key lookup.

ElasticCache is really a Memchache Cluster; is a distributed collection of cache nodes. This nodes are unfortunately abailable
only in AZ. And that AZ can be a single qualifier, especially if you are using it for ussing sessions.

Customers need multi-AZ HA (high Avaliability) must roll on their on.

Cache nodes are used on their on.

Cache nodes are used-specified types - Very much like EC2 instance types. they can be even been wreap with security groups, and can
buy "capacity reservations". the are also resizable, which cna be doed, by addign / removing larger nodes.

To set an Elastic Cache, you have to choose an AZ. choose a node type, and the number of nodes. Launch the cluster, Connect your
application by pointng to the Elastic Cache endpoint. Then benchmark and resize if necesary. Once capacity is kown purchase reservations
to save money. 

Expiring keys is controlled via  the elastic ElasticCache API, (memcache commands):
This can be done:
  - squedule 
  - on demand.
  
Elastic Cache, has an hourly charge: Reseverd or On demand. See "Pricingz model for EC2" for reference.

There is also Bandwidth charges
   - Free inside the AZ.
   - And $0.1 GB, in and aout accross AZs.
   

INTRODUCTION TO VPC:
====================

VPC stands for Virtual Private Cloud. VPC allows a logical isolation between your resources: From other constumers, and from internal 
divisions or application tiers.
It gives you control over:
 - adress spaces.
 - subnets.
 - Route tables & NATs (Netword Address Translations)
 - Network gateways (VPN and Internet).
 - Ingress and Engress security groups.
 - Multiple NICs (Elastic Network Interfaces or ENIs) on VPC EC2s.
 - Dedicated Hardware (with EC2 only).
 
Control over route tables & NATs
 A route table, sets what traffic goes down which line. - Depends on sender and target - 
 
NATs (Network Adress Trasnlation)- Lets machines "hide" on private network, but still be able to get out of the network. 
AWS provides a NAT instance for that porpouse.

VPC, also gives you control over Gateways; an Internet Gateway (IG), allows VPC resources to access Internet, is the only way for 
the resources to connect to Internet. It can be connected/disconnected programmatically.- For example when you want to deploy patches
in your EC2 isntances, you can put down you IG, pull down your patches, and then reconnect.

There is also a VPN Gateway (VPG)- this is a Harware solution that mates with on-premise VPN. Your on-prem must obey Border Gateway 
protocol o "BGP", most of the VPN appliances do. This can be used to treat VPC as an extension of corporate. This comes with an 
additionally hourly cost. 

Software VPN is also possible. You can use OpenVPN (free) is a popular solution.

VPC allows a logical isolation of your resources:
 - from other customers.
 - from internal divisions/application tiers. Many time when you are developing applications, the security applications require that
 the database is in a separate subnet. VPC allows for this, while amazon classic does not.
 
 VPC allows control over address spaces:
 - Provision an entire "B" Class of IP if you like (255.255.x.x) or "/16" - 65.000 ips
 - Any adress space provided in VPC will be available in subnets.
 - VPC adresses don't change when EC2 bounced.
 - Public-facing IPs still need to be Internet routable.
  + Can use Elastic IP.
  + You must route through Internet Gateway.
  
VPC allows control over subnets
 - Subnets provisioned in single AZ.
 - Can have 200 subnets, per VPC.
 - In VPC, think of subnets as AZs.
   · Autoscaling groups can span VPC subnets.
 - The Minimun size of a subnet must be a /28 (14 IP adresses)
 - The Maximun size = size of the VPC.
 - When sharing with the outside network (like bursting from corporate) watch out for IP conflicts.
 
 VPC also gives you control over route table or NATs
 - Route table: "what traffic goes down which line" - depends on sender and target.
 NAT(Network Adress Trasnlation) lets machines "hide" on private network, but still be able to get out fo network.
 AWS provides a NAT instance for this pourpouse.
 
 VPC gaves you control over network gateways.
 - Internet gateways (IG): Allows VPC resources to access the internet.
 - And is the only way for this resources to access the internet.
 - Can be connected/disconnected programatically (for example when you are doing patch updates on your EC2 servers, you can connect
 your IG, pull down your patches, and then disconnect your IG.)
 
 There is also a VPN Gataway or VPG: this is a hardware VPN solution mates with on-premise VPN. Your On-prem must obey BGP, or
 Border Gate Protocol.
 
 [5:13]

 
 
 
 
 
    





