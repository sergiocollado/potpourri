AWS: S3 - Simple Storage Service.

ref: https://aws.amazon.com/es/documentation/s3/

INTRODUCTION:
============

you can put any data file in S3 (Simple Storage Service)
they are organiced in "Buckets".
-Bucket names are globally unique, so you have to take one with a name non taken yet.

easy to DNS CNAME a URL to your bucket.
bucket name: sergio-images
but my desired domein is sergio.collado.com 

so: sergio.collado.com -> collado-images.s3.amazonaws.com

and include in the web, as:

<html>
<body>
<img src="http://sergio.collado/path/to/logo" alt="AWS_lOGO">
...

ACCESS CONTROL:
==============

Objects, also have 4 types of access control:
 · IAM policies
 · Bucket policies
 · ACLs(Acess Control Lists) - AWS console
 · Query string autentification - gives expiring acess.
 
 
 more info about Buckets in: http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingBucketPolicies.html
 more info about ACLs in: http://docs.aws.amazon.com/AmazonS3/latest/dev/S3_ACLs_UsingACLs.html
 
 The bucket also saves some information about the files: type, creation date, ...
 more info in: http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingMetadata.html
 
STORING POLICIES:
=================

There are three ways to store policies:
 · Standard ( "11 9's") - that means that if you have 1 million files stored there, you will loose one, everty 10.000 years.
   · it comes at greatest cost, but is the default.
 · Reduced Redundancy Storage (RRS) - is ("4 9s") - it means that if you have 10000 files there, you'll probable loose one every year.
   · It cost is reduced about 20% / Great for reproducible assets
   · RRS, can trigger notifications, on "object missing". 
 · Glaciar it has ("11 9's), but is great draw back is queued retrieval -you cannot get it back in real time- 
   · But is cost is about the 10% of the standard option - which make a great choise for arcivals, and back-ups.
   
PRICING:
========

Starts around 0.10$ per GB/month for "standard" S3 - the fee gets cheaper, if you use more.

Bandwith:
  IN: FREE
  OUT: Same Region: free
       Out of region: Starts at 0.12$ GB/month.
  REQUEST al also costy: (PUT, GET, POST, LIST) - Starts at 0.0055/1000 request.
   
 Just have in mind, that data trasnsfer inside your region is free!  
   
   
ADVANCED FEATURES OF S3
=======================

 S3 also allows for:
 
  ·light, static, website hosting.
  ·Object squeduled expiration.
  ·Encryption Options
  ·Versioning
  ·Logging
  ·"Direct from browser" upload to s3
  ·Multipart files upload
  ·Requested pays
  ·BitTorrent Support
   
- Simple static website hosting: You can rename to a domain you own  (Easy CNAMEd).
- Index, and Error documents support.
- key prefix routing rules.
- Oject expiration squeduling -live cicle management- 
- Encription options (SSE, SSL, or your own) [SSE: Server Side Encription]
- Versioning. Once is On, it can never can go back to Off, so you will be storing all your development. ##
- Logging: you can get IP, operations, dates, . Can be On/Off anytime. Watch out! store the logs, in another Bucket!
- Direct on brownser upload: You can grant users to directly upload to your bucket uses IAM/STS services to generate a url for
the bucket object. 5GB limitation on file size. ref -> http://aws.amazon.com/articles/1434
 If you want to upload more than 5GB, you have to use multi-part files upload, that is just limited to SDK(Java, iOs,PHP, Ruby); 
 the client chuncks the file into pieces. Loads each piece separately. Client closes file. S3 reassembles the file. This also allows
 stop/star partial uploads. With this method the max file size is 5TB.
 ref: http://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html
 - Request payment. always if the client is a AWS registered partner/customer.
 - Bittorret support (P2P protocoll) max file size 5GB
 ref: http://docs.aws.amazon.com/AmazonS3/latest/dev/S3Torrent.html
 
 
 ·First steps: 
 
The best is to migrate all static content to S3 (images, CSS, Javascript, HTML, etc...)- this reliebes the load on your
EBS, so it can do more important tasks, with this, the cost of the EBS, is much more lower, and gets more performance.
   
The second step, is set up Glacier for archival, Control access, set up Livecicle policies, Investigate advanced users: 
like Website hosting, direct to S3 uploads.
 



log in main server server:

|___|  __|_  )
|_|   (     /
|___| ___||___|

<< sudo su -
cd /var/local/src

Gon ; s3tools.org/download

--íntasll from the commmand, get the link

 wget http:.... (he link copied proivied)
 
 >> mv download s3cmd.trg
 
  tax -xcvzf s3cmd.tgz
  
  cd s3cmd-1.5.0-alphay/
  
  Less install
  
  easteisnt way to install is : python .py instals
 
  s3cmd-1.5.0-alpha1
  
  
  we have provide the kees> using > credential needed, > access credenctiasn "Aceess Keys´2"
  
 
  
  intall the keys. 
  
  w3cdmd --help 
  s3cmd [feature] --help 
  
  
INTRODUCTION TO CLOUD FRONT:
============================

-> Cloud Front is a CDN - Content Delivery Network.
   It uses locations close to end user (Edges).
   Other populars CDNs are Akamai, Level 3.

DESCRIPTION:
===========

- Resources of static website assests: Images, CSS, JS, HTML, JSON.
- Distributions: products, documents, data.
- Streaming: audio, video.
    Streaming uses propietary protocols, unlike http. Cloud Front uses Adobe/macromedia RTMP (Real Time Messaging Protocol), also the is
    an encrypted version: RTMPE (Real Time Messaging Protocol Encrypted)
    
 This is natively integrated in AWS, you can specify S3 Buckets as origins. Its interface is very easy, also supports standard http/s
 also streaming with RTMP/E, and is very cost effective: Contents are only cached, at edges, when there is a user request, and you can 
 free that chache, on demand or on squedule.
 
  Data can be "expired":
     - On squedule
     - On demand.
 
 HOW TO SET A CLOUDFRONT:
 ========================
 
 First you have to define a "Distribution":
    - you have to specify if this will be HTTP, or Streaming.
    - Set an 'origin': any web-accesible web server will do: private server, EC2 or S3.
    - You can also specify: 
              - TTL (Time To Live)- this defines how often Cloud Front is going to check for a new version of the file.
              - Path matches - this specifies what parts of your origin web server are going to be cached.
              - Logging - allows log request to Cloud Front.
              
 At this time you have created a distribution domain, you can easily rename using CNAMEd "vanity URLs" - this is useful in cases as
 Websites (CSS, JS, images references) or Applications (media players, web, mobile, or desktop apps).
 
 FINAL USER ACCESS:
 ==================
 
 When a final user access to a CDN (Content Delivery Network) URL, CloudFront directs the user to the closes edje, so it serves
 the data faster.
 This is a pulling mechanism, so if the closes edje to a client, doesn't have the required data, that edje request the data to the
 the origin edge, and pull that data down local, so next time a client request the data in that location, it will already stored 
 in the closest edje, and serviced faster.
 
  
 
 
 
    




