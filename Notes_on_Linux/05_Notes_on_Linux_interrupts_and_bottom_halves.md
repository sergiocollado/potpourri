# Notes on Linux interrupt and bottom halves


## Interrupts 

references:
- MUST READ LDD3: https://static.lwn.net/images/pdf/LDD3/ch10.pdf
- IRQs: the Hard, the Soft, the Threaded and the Preemptible: https://youtu.be/-pehAzaP1eg
- Kernel Recipes 2016 - Understanding a Real-Time System (more than just a kernel) - Steven Rostedt: https://youtu.be/w3yT8zJe0Uw
- https://www.kernel.org/doc/html/latest/core-api/genericirq.html
- https://0xax.gitbooks.io/linux-insides/content/Interrupts/linux-interrupts-1.html
- https://cdrdv2.intel.com/v1/dl/getContent/671190
- https://linux-kernel-labs.github.io/refs/heads/master/lectures/interrupts.html
- https://sysprog21.github.io/lkmpg/#scheduling-tasks
- https://sysprog21.github.io/lkmpg/#interrupt-handlers
- https://en.wikipedia.org/wiki/Interrupt_vector_table
- https://en.wikipedia.org/wiki/Interrupt_handler
- https://tldp.org/HOWTO/Plug-and-Play-HOWTO-7.html
- https://people.freebsd.org/~jhb/papers/bsdcan/2007/article/article.html
- https://en.wikipedia.org/wiki/Advanced_Programmable_Interrupt_Controller
- https://elixir.bootlin.com/linux/v6.5.7/source/include/linux/interrupt.h
- https://elixir.bootlin.com/linux/v6.5.7/source/arch/x86/kernel/irq.c
- https://linux.die.net/HOWTO/KernelAnalysis-HOWTO.html#toc1
- https://docs.kernel.org/core-api/irq/index.html
- https://www.oreilly.com/library/view/understanding-the-linux/0596005652/ch04s06.html
- https://linux.ime.usp.br/~marcelosc/2019/09/Introduction-to-IIO-driver-development


### What is an interrupt?

- reference: https://docs.kernel.org/core-api/irq/concepts.html
- reference: https://www.kernel.org/doc/html/latest/core-api/genericirq.html
- reference: https://0xax.gitbooks.io/linux-insides/content/Interrupts/linux-interrupts-1.html
- reference: https://embetronicx.com/tutorials/linux/device-drivers/interrupts-in-linux-kernel/

An interrupt is an input signal to the processor, sent by the hardware peripherals when they need processor attention. 

An IRQ is an interrupt request from a device. Currently they can come in over a pin, or over a packet. Several devices may be connected to the same pin thus sharing an IRQ.

An IRQ number is a kernel identifier used to talk about a hardware interrupt source. Typically this is an index into the global irq_desc array, but except for what linux/interrupt.h implements the details are architecture specific.

An IRQ number is an enumeration of the possible interrupt sources on a machine. Typically what is enumerated is the number of input pins on all of the interrupt controller in the system. In the case of ISA what is enumerated are the 16 input pins on the two i8259 interrupt controllers. The irq numbers are defined by the architecture of the system's processor: https://en.wikipedia.org/wiki/Interrupt_request#x86_IRQs

Architectures can assign additional meaning to the IRQ numbers, and are encouraged to in the case where there is any manual configuration of the hardware involved. The ISA IRQs are a classic example of assigning this kind of additional meaning.


### What is the purpose of the interrupts?

For example, if we want to perform an action with an incomming packet from the network card as soon as the packet arrives.

If you don't want to continously ask the network card "has any packet arrive yet?" and waste processor time, you
can use external hardware interrupt IRQ

The interrupt line from a device should be connected to the INTR line of the CPU, and after each packet is received,
the network card will make a signal over this line. 

The CPU will sense this line and know that the network card has information for it.

Only after that, the CPU will read the incoming packet. 

### Types of interrupt

1. - Hardware / asynchronous:
     - Generated by hw devices
     - Occours at arbitrary times (asynchronous) with respect to the clock signals
     - Examples: processing of a key on the keyboard, mouse movement, timer fired, network card reporting arrival of a packet...
2. - Software / synchronous:
     - Generated by executing instructions
     - Occours synchronously with respect to processor clock
     - Also called as exceptions/traps
     - Examples: divide-by-zero, system call, page fault


#### Hardware interrutps
 - reference: https://www.kernel.org/doc/html/latest/kernel-hacking/hacking.html#hardware-interrupts-hard-irqs

Timer ticks, network cards and keyboard are examples of real hardware which produce interrupts at any time. The kernel runs interrupt handlers, which services the hardware. The kernel guarantees that this handler is never re-entered: if the same interrupt arrives, it is queued (or dropped). Because it disables interrupts, this handler has to be fast: frequently it simply acknowledges the interrupt, marks a 'software interrupt' for execution and exits.

You can tell you are in a hardware interrupt, because `in_hardirq() returns true.

#### Software interrupts 
 - reference: https://www.kernel.org/doc/html/latest/kernel-hacking/hacking.html#software-interrupt-context-softirqs-and-tasklets

Whenever a system call is about to return to userspace, or a hardware interrupt handler exits, any 'software interrupts' which are marked pending (usually by hardware interrupts) are run (kernel/softirq.c).

Much of the real interrupt handling work is done here. Early in the transition to SMP, there were only 'bottom halves' (BHs), which didn't take advantage of multiple CPUs. Shortly after we switched from wind-up computers made of match-sticks and snot, we abandoned this limitation and switched to 'softirqs'.

include/linux/interrupt.h lists the different softirqs. A very important softirq is the timer softirq (include/linux/timer.h): you can register to have it call functions for you in a given length of time.

Softirqs are often a pain to deal with, since the same softirq will run simultaneously on more than one CPU. For this reason, tasklets (include/linux/interrupt.h) are more often used: they are dynamically-registrable (meaning you can have as many as you want), and they also guarantee that any tasklet will only run on one CPU at any time, although different tasklets can run simultaneously.

    
### Exceptions

Exceptions are classified as faults, traps, and abort, depending on the way they are reported and whether the instruction 
that caused the exception can be restared, without loss of programs. 

Traps increment the instruction pointer, faults don't, and aborts "explode". 

- Faults: These can be corrected and the programmer may continue as if nothing happened. Eg. page fault 
- Traps: Traps are reported inmediately after the execution of the trapping instruction. Eg. int instruction
- Aborts: Some severe unrecoverable error. Eg. hardware failure

### General protection Fault

A general protection fault may occur for various reasons, the most common:
 - segment error (privilege, type, limit, read/write rights).
 - executing priviledged instructions while CPL (current privilege level) is not equal to 0.
 - writing a 1 in a reserved register field.
 - referencing or accessing a null-descriptor.
 - trying to access an unimlemented register (like mov cr6, eax)
 - the saved instruction pointer points to the instruction which causes the exception


### HW datasheets examples about interrupts: 

 - reference: HW Intel interrupts: https://cdrdv2.intel.com/v1/dl/getContent/671190
 - reference: HW Amd64 interrupts: https://www.scs.stanford.edu/05au-cs240c/lab/amd64/AMD64-2.pdf

![interrupt_vector_source_and_cause](https://github.com/sergiocollado/potpourri/blob/master/Notes_on_Linux/images/amd64_interrutp_vector_source_and_cause.png)

![interrupt_vector_classification](https://github.com/sergiocollado/potpourri/blob/master/Notes_on_Linux/images/amd64_interrutp_vector_classification.png)

![interrupt_vector_classification_cont](https://github.com/sergiocollado/potpourri/blob/master/Notes_on_Linux/images/amd64_interrupt_vector_classification_cont.png)


#### Example 1
So for example for a divide by zero problem:

```c
  // divide_by_zero.c
  1 #include <stdio.h>                                                               
  2                                                                                  
  3 int main()                                                                       
  4 {                                                                                
  5 »       int i = 1/0;                                                             
  6 »       return 0;                                                                
  7 }                                                                                 
```

```bash
sergio@laptop:~/repos/divide-by-zero$ gcc -o div_by_zero.c divide_by_zero.c 
divide_by_zero.c: In function ‘main’:
divide_by_zero.c:5:18: warning: division by zero [-Wdiv-by-zero]
    5 |         int i = 1/0;
      |           
```
what should happen here?

```bash
sergio@laptop:~/repos/divide-by-zero$ ls
div_by_zero.c  divide_by_zero.c
sergio@laptop:~/repos/divide-by-zero$ sudo dmesg -WH &
[1] 6310
sergio@laptop:~/repos/divide-by-zero$ ./div_by_zero.c 
[oct 8 16:43] traps: div_by_zero.c[6315] trap divide error ip:55d9a6c7b13c sp:7ffc0f832c10 error:0 in div_by_zero.c[55d9a6c7b000+1000]
Floating point exception (core dumped)
```

#### Example 2

```c
  //  userapp.c 
  1 #include <stdio.h>                                                               
  2 #include </usr/include/sys/io.h>                                                 
  3                                                                                  
  4 int main()                                                                       
  5 {                                                                                
  6 »       outb(0x80, 0x00);                                                        
  7 »       return 0;                                                                
  8 }   
```

```
sergio@laptop:~/repos/divide-by-zero$ gcc -o userapp userapp.c peers lock
sergio@laptop:~/repos/divide-by-zero$ sudo dmesg -WH &
sergio@laptop:~/repos/divide-by-zero$ ./userapp
[oct 8 19:56] show_signal: 1 callbacks suppressed
[  +0,000006] traps: userapp[4589] general protection fault ip:55c13b8c4140 sp:7fff1bd28c70 error:0 in userapp[55c13b8c4000+1000]
Segmentation fault (core dumped)
```

In this case we are attemping to write to a memory that it is not allow, we don't have access writes, so a general protection fault is triggered.


We can see it should be a fault, but in linux is denominated as a trap. 

### How debuggers work?

To implement breakpoints on the x86 architecture, software interrupts (also known as "traps") are used.

Breakpoints are implemented on the CPU by a special trap called int 3.

`int` is 0x86 jargon for "trap instruction" - a call to a predefined interrupts handler. 

x86 supports the int instruction with a 8-bit operand specifying the number of the interrupt that occurred.

```c
  1 #include <stdio.h>                                                               
  2                                                                                  
  3 int main() {                                                                     
  4 »       int i = 0;                                                               
  5 »       while (i < 6) {                                                          
  6 »       »       printf("i equals to: %d\n", i);                                  
  7 »       »       ++i;                                                             
  8 »       }                                                                        
  9 »       __asm__("int3");                                                         
 10 }                                                                                            
```

```bash
sergio@laptop:~/repos/divide-by-zero$ gcc -o int3 interrupt.c 
sergio@laptop:sudo dmesg -WH &
i equals to: 0
i equals to: 1
i equals to: 2
i equals to: 3
i equals to: 4
i equals to: 5
[  +1,920005] traps: int3[7075] trap int3 ip:55f6a6900182 sp:7ffdc37606c0 error:0 in int3[55f6a6900000+1000]
Trace/breakpoint trap (core dumped)
```

but if the program is debugged with gcc: 

```
sergio@laptop:~/repos/divide-by-zero$ gdb ./int3 
GNU gdb (Ubuntu 12.1-3ubuntu2) 12.1
Copyright (C) 2022 Free Software Foundation, Inc.
License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>
This is free software: you are free to change and redistribute it.
There is NO WARRANTY, to the extent permitted by law.
Type "show copying" and "show warranty" for details.
This GDB was configured as "x86_64-linux-gnu".
Type "show configuration" for configuration details.
For bug reporting instructions, please see:
<https://www.gnu.org/software/gdb/bugs/>.
Find the GDB manual and other documentation resources online at:
    <http://www.gnu.org/software/gdb/documentation/>.

For help, type "help".
Type "apropos word" to search for commands related to "word"...
Reading symbols from ./int3...

This GDB supports auto-downloading debuginfo from the following URLs:
https://debuginfod.ubuntu.com 
Enable debuginfod for this session? (y or [n]) y
Debuginfod has been enabled.
To make this setting permanent, add 'set debuginfod enabled on' to .gdbinit.
(No debugging symbols found in ./int3)
(gdb) run
Starting program: /home/sergio/repos/divide-by-zero/int3 
[Thread debugging using libthread_db enabled]
Using host libthread_db library "/lib/x86_64-linux-gnu/libthread_db.so.1".
i equals to: 0
i equals to: 1
i equals to: 2
i equals to: 3
i equals to: 4
i equals to: 5

Program received signal SIGTRAP, Trace/breakpoint trap.
0x0000555555555182 in main ()
(gdb) 
```
the program was ran, and stoped by itself at the int3 (breakpoint trap)

if we continue the program, it will exit normally:

```
Program received signal SIGTRAP, Trace/breakpoint trap.
0x0000555555555182 in main ()
(gdb) c
Continuing.
[Inferior 1 (process 7187) exited normally]
```

### Hardware interrupt triggering methods

Each interrupt signal input is designed to be triggered by either a logical signal event or particular edge (level transition)

#### Level triggered 

Interrupt is requested by holding the interrupt signal at its particular (high or low) active logic level.
Level triggered interrupts happen as long as the interrupt line is held active level. 

As long as the line, you get interrupt, when you serve the interrupt and return, it the interrupt line is still 
active, you get the interrupt again inmediatelly. 

#### Edge triggered

Interrupt is requested by a level transition on the interrupt line. 
 - Falling edge (high to low)
 - Rising edge (Low to high)

These interrupts are issued per transition and not repeated. E.g. in networking when the packet queue goes from
empty to non-empty. This makes it critical to never miss an edge triggered interrupt, because failure to handle
one interrupt result in no further interrupts from happening. 

### Masking

Processors typically have an internal interrupt mask register. This allows selective enabling and disabling 
of hardware interrupts. 

Each interrupt signal is associated with a bit in the mask register
- Bit = 1, Interrupt enabled
- Bit = 0, Interrupt disabled

When the interrupt is disabled, the associated interrupt signal will be ignored by the processor. 

Maskable interrupts: Interrupts which can be enabled/disabled.

Non maskable interrupts: Interrupts which cannot be disabled. Example: NMI, timeout signal from watchdog timer. 

For example the 8086 processor has two hardware interrupt signals:

- NMI non-maskable interrupt
- INTR Interrupt request (maskable interrupt)

How to support more than two interrupts?

It would be very unproductive to make a ton of INTR pins on the CPU for all of them.

To solve this problem a special chip was invented, the interrupt controller. 

### Programable interrupt controller (PIC) 

The Intel 8259 Programmable Interrupt Controller (PIC) is one of the most important chips making the x86 architecture.

reference: https://en.wikipedia.org/wiki/Intel_8259

It allows multiplexing the single INT line on the x86 processor to multiple interrupts lines.
Interrupt lines can be assigned various hardware priority levels, as well as a programmable interrupt masking.

Each PIC has 8 interrupts lines called Interrupt ReQuests (IRQ), numbered from 0 to 7. 
PIC has one output line which connects the INTR line to the CPU. 

A device supporting interrupts has an output pin used for signalling an Interrupt ReQuest (IRQ)

CPU will know that some devices requires its inmediate attention, and the processor will ask the PIC which 
of the 8 input lines (IRQx) was the source of the interruption.

#### Why call Interrupt ReQuest (IRQ)

Peripheral devices cannot directly force interrupts, but has to request them via the PIC, we call them IRQ
or Interrupt ReQuests. 

#### Dual PIC

Soon 8 lines were not enough. To increase the total number of interrupt lines two 8259 PIC controllers
(master and slave) were connected in cascade (Dual PIC). 

- IRQs from 0 to 7 where processed with the first Intel 8259 PIC (master)
- IRQs from 8 to 15 are processed with the second Intel 8259 PIC (slave)

Only the master is connected to the CPU and can signal about the incomming interrupts.
It there is an interrupt on lines 8-15, the second PIC (slave) will signal about it to the master
on the line IRQ 2, and after that the master will signal the CPU. 

#### Port Interface

Two separate dedicated ports in the x86 IO-port space for each connected PIC

- Master PIC - 0x20, 0x21
- Slave  PIC - 0xA0, 0xA1

```
# sudo cat /proc/ioports | grep -i pic # you really need to use sudo!
```

#### Device interrupt mapping

Devices were initially statically assign, so the assignations were: 

The number assigned to the irq depends on the system architecture: https://en.wikipedia.org/wiki/Interrupt_request#x86_IRQs

##### Master PIC
- IRQ 0 — system timer
- IRQ 1 — keyboard controller
- IRQ 2 — cascade (interrupt from slave controller)
- IRQ 3 — serial port COM2
- IRQ 4 — serial port COM1
- IRQ 5 — parallel port 2 and 3 or sound card
- IRQ 6 — floppy controller
- IRQ 7 — parallel port 1

##### Slave PIC 
- IRQ 8 — RTC timer
- IRQ 9 — ACPI
- IRQ 10 — open/SCSI/NIC
- IRQ 11 — open/SCSI/NIC
- IRQ 12 — mouse controller
- IRQ 13 — math co-processor
- IRQ 14 — ATA channel 1
- IRQ 15 — ATA channel 2

#### Programmable Interrupt Request (PIRQ)

Initially, the x86 used ISA Bus (https://en.wikipedia.org/wiki/Industry_Standard_Architecture).

The PCI bus, later replaced the ISA bus. Unfortunately the number of devices began to exceed the number 15. 
Also instead of the static ISA bus, devices in the PCI bus can be added to the system dynamically. 
Interrupts in the PCI bus can be shared, so it is possible to connect many devices to one interrupt line IRQ. 

In the end, to solve the problem of lack of interrupt lines, it was decided to group interrupts from all 
the PC devices to PIRQ lines (Progammable Interrupt ReQuests). 

For example, suppouse we have 4 free interrupts lines on the PIC controller and 20 PCI devices. 
We can combine interrupts from 5 devices into one PIRQx line, and connect those PIRQx lines to the PIC controller.
In this case if there is an interrupt on the PIRQx lines, the processor will have to ask all the devices 
connected to this line about the interrupt to know who is responsible for it, but in the end it solves 
the problem. 

The device that connects the PCI interrupt lines to PIRQ lines is often called a PIR router. 
This method ensures that PIRQx interrupt lines don't connect to lines with ISA interrupts (since 
this will produce conflicts). System software, such as the BIOS or operating system, is
responsible for programming the interrupt router. 

- reference: https://tldp.org/HOWTO/Plug-and-Play-HOWTO-7.html
- reference: https://people.freebsd.org/~jhb/papers/bsdcan/2007/article/article.html

#### APIC (Advanced PIC)

The PIC method only works for a single processor systems.

PIC can only send interrupts to one CPU, and in a multiprocessor system it is desired to load CPUs in a balanced way.

The solution to this problem was the new APIC interface (Advanced PIC).

It is comprised of two components: 
 - IO-APIC - Interfaces with Devices.
 - LAPIC - the local APIC - Interfaces with CPU.

reference: https://en.wikipedia.org/wiki/Advanced_Programmable_Interrupt_Controller

##### LAPIC 

Each processor in a multiprocessor system consists on a LAPIC (local APIC). Responsible for: 
- receiving various interrupts requests and delivering them to the processor.
- handling prioritization of interrupts
- sending interrupts to other processors (known as inter processor interrutps or IPIs)

LAPIC can be connected directly to I/O devices via local interrupt inputs (timer, thermal sensor) or
throught IOAPIC via external interrupts. 

LAPIC can generate interrupts due to interrupt requests received from various sources: 
 - IPIs received from other processors.
 - Interrupts coming from LINT or EXTINT

##### I/O PIC

Connects to the devices to allow device interrupt requests to be routed to LAPIC(s).
There can be one or more IOAPIC in the system. Each IOAPIC has 24 interruption lines. 
IOAPIC receives interrupts requests from the devices and sends them to LAPIC(s) based
upon the redirection table entries (RTE) programmed in the IOAPIC. 

Note: to maintain backward compatibility, APIC emulates 8259 PIC. 

###### Detection

The CPUID.01h:EDX[bit 9] flag specifies whether a CPU has a build-in local APIC. 


#### CPUID

CPUID is an x86 opcode which stands for CPU Identification.

The CPUID instruction can be used to retrieve various amount of information about your cpu: 
 - vendor string
 - model number
 - size of the internal caches
 - list of CPU features

```
man 4 cpuid
```

The cpuid driver is not auto-loaded. On modular kernels you might need to use the following 
command to load it explicitly before use: 

```
modprobe cpuid
```

Most of the information in cpuid is reported by the kernel in cooked form either in /proc/cpuinfo

```
cat /proc/cpuinfo | grep - i apicid
```
apicid: An unique ID given to each logical processor upon startup. 

We can check on the system log, the "apic" messages: 

```
sudo dmesg | grep -i apic
```

### What happens when there is an interrupt?

The device asserts IRQ of I/O APIC. 

I/O APIC transfer interrupt to LAPIC. 

LAPIC asserts CPU interrupts.

After current instrucction completes CPU senses interrupt line and obtains the IRQ number from LAPIC, jumps to the interrupt handler. 


### How does the hardware find the interrupt handler? 

#### Interrupt vector (IV)

On x86 each interrupt or exception is identified by a number between 0 and 255. Intel calls this number a vector. 

The interrupt vector is used by the interrupt-handling mechanisms to locate the system-software service routine
assigned to the exception or interrupt.

Up to 256 unique interrupt vectors are available in x86.

The number or interrupt vectors or entry points supported by a CPU differs based on the CPU architecture. 

The first 32 vector entries are reserved for predefined exceptions and interrupt conditions. 

Look into arch/x86/include/asm/traps.h. For example: https://elixir.bootlin.com/linux/latest/source/arch/x86/include/asm/trapnr.h

```
// https://elixir.bootlin.com/linux/latest/source/arch/x86/include/asm/trapnr.h
/* SPDX-License-Identifier: GPL-2.0 */
#ifndef _ASM_X86_TRAPNR_H
#define _ASM_X86_TRAPNR_H

/* Interrupts/Exceptions */

#define X86_TRAP_DE		 0	/* Divide-by-zero */
#define X86_TRAP_DB		 1	/* Debug */
#define X86_TRAP_NMI	         2	/* Non-maskable Interrupt */
#define X86_TRAP_BP		 3	/* Breakpoint */
#define X86_TRAP_OF		 4	/* Overflow */
#define X86_TRAP_BR		 5	/* Bound Range Exceeded */
#define X86_TRAP_UD		 6	/* Invalid Opcode */
#define X86_TRAP_NM		 7	/* Device Not Available */
#define X86_TRAP_DF		 8	/* Double Fault */
#define X86_TRAP_OLD_MF	         9	/* Coprocessor Segment Overrun */
#define X86_TRAP_TS		10	/* Invalid TSS */
#define X86_TRAP_NP		11	/* Segment Not Present */
#define X86_TRAP_SS		12	/* Stack Segment Fault */
#define X86_TRAP_GP		13	/* General Protection Fault */
#define X86_TRAP_PF		14	/* Page Fault */
#define X86_TRAP_SPURIOUS	15	/* Spurious Interrupt */
#define X86_TRAP_MF		16	/* x87 Floating-Point Exception */
#define X86_TRAP_AC		17	/* Alignment Check */
#define X86_TRAP_MC		18	/* Machine Check */
#define X86_TRAP_XF		19	/* SIMD Floating-Point Exception */
#define X86_TRAP_VE		20	/* Virtualization Exception */
#define X86_TRAP_CP		21	/* Control Protection Exception */
#define X86_TRAP_VC		29	/* VMM Communication Exception */
#define X86_TRAP_IRET	        32	/* IRET Exception */
#endif
```

```
 NMI:     Non-maskable interrupts
 LOC:     Local timer interrupts
 SPU:     Spurious interrupts
 PMI:     Performance monitoring interrupts
 IWI:     IRQ work interrupts
 RTR:     APIC ICR read retries
 RES:     Rescheduling interrupts
 CAL:     Function call interrupts
 TLB:     TLB shootdowns
 TRM:     Thermal event interrupts
 THR:     Threshold APIC interrupts
 DFR:     Deferred Error APIC interrupts
 MCE:     Machine check exceptions
 MCP:     Machine check polls
 PIN:     Posted-interrupt notification event
 NPI:     Nested posted-interrupt event
 PIW:     Posted-interrupt wakeup event

```


#### Interrupt descriptor table (IDT)

The IDT (Interrupt Descriptor Table) is a linear table of 258 entries which associates an interrupt handler with each interrupt vector. 

When an interrupt is fired, the CPU looks at the IDT table, and finds what method needs to be called. 

Each descriptor is of size 8 bytes (on x86) and 16 bytes (on x86_64)

During early boot, the architecure-specific branch of the kernel code sets up the IDT in memory and programs
the IDTR register (special 0x86 register) of the processor with the physical start address and length of the IDT. 


## Interrupt handling in Linux Kernel

- reference: https://linux-kernel-labs.github.io/refs/heads/master/lectures/interrupts.html#interrupt-handling-in-linux
- reference: https://www.cs.montana.edu/courses/spring2005/518/Hypertextbook/jim/media/interrupts_on_linux.pdf

1. Whenever an interrupt occurs, assembly instructions in linux kernel are executed, which
locates relevant vector descriptor by multiplying reported vector number by size of vector number(8/16)
and adding the result to the base address of IDT.

2. common_interrupt: [arch/x86/entry/entry_64.S](https://elixir.bootlin.com/linux/v6.5.7/source/arch/x86/entry/entry_64.S):
	- saves the context of the running process
	- This includes instruction pointer (IP), stack pointer (SP) and other registers needed to resume the process again
	- This context is usually saved on the stack.
	- Then the context is changed to interrupt stack.

reference: https://elixir.bootlin.com/linux/v6.5.7/source/arch/x86/entry/entry_64.S

2. Finally it arrives at `do_IRQ()`. `do_IRQ()` is the common function for all hardware interrupts

```
	arch/x86/kernel/irq.c
```
 - reference: https://elixir.bootlin.com/linux/v6.5.7/source/arch/x86/kernel/irq.c

3. Finds IRQ number in saved %EAX register

4. Calls handle_irq which will finally call our registered interrupt handler.

### Interrupt statistics

To check the interrupt statistics: 

```
$ cat /proc/interrupts
```
or to see how those change dinamically:

```
$ watch -d -n 1 cat /proc/interrupts
```

or use the `procinfo` -  display system statistics gathered from /proc

```
$ procinfo
```

 - reference: https://linux.die.net/man/5/proc

```
# cat /proc/interrupts
```
This is used to record the number of interrupts per CPU per IO device. Since Linux 2.6.24, for the i386 and x86_64 architectures, at least, this also includes interrupts internal to the system (that is, not associated with a device as such), such as NMI (nonmaskable interrupt), LOC (local timer interrupt), and for SMP systems, TLB (TLB flush interrupt), RES (rescheduling interrupt), CAL (remote function call interrupt), and possibly others. Very easy to read formatting, done in ASCII.

```
$ cat /proc/interrupts

           CPU0       CPU1       CPU2       CPU3       CPU4       CPU5       
   0:          2          0          0          0          0          0   IO-APIC    2-edge      timer
   1:        458          0          0          0       6154          0   IO-APIC    1-edge      i8042
   8:          0          0          0          0          0          1   IO-APIC    8-edge      rtc0
   9:          0          0          0          0          0          0   IO-APIC    9-fasteoi   acpi
  12:          0          0          0      22185          0          0   IO-APIC   12-edge      i8042
  14:          0          0          0          0          0          0   IO-APIC   14-edge      ata_piix
  15:          0          0          0          0          0          0   IO-APIC   15-edge      ata_piix
  16:          0      19541          0          0          0          0   IO-APIC   16-fasteoi   vmwgfx, snd_ens1371
  17:       9825          0      47519          0          0          0   IO-APIC   17-fasteoi   ehci_hcd:usb1, i
```

 - Column 1:  IRQ number the file shows only interrupts corresponding to installed handlers. The irq numbers are assigned depending on the systems CPU architecture: https://en.wikipedia.org/wiki/Interrupt_request#x86_IRQs

 - Column 2:  counter of the number of interrupts received. A column is present for each processor on the system

 - Column 3/4: Type of the interrupt and device that handles the interrupt.
           -  For x86.
           -  XT-PIC — This is the old AT computer interrupts. 8259
           -  IO-APIC 

 - Column 5:  device associated with this interrupt. This name is supplied by the devname parameter to request_irq(),
            


#### Difference between  IO-APIC-fasteoi and IO-APIC-edge?

The difference lies in the way the interrupts are triggered.

The `-edge` interrupt are edge triggered.

The `-fasteoi` interrupts are level interrupts that are triggered until the interrupt event is acknowledged in the programmable interrupt controller (PIC). 

The EOI stands for End Of Interrupt.

### Watch Interrupts

To see the interrupts occurring on your system, run the command:

```
$ watch -d -n1 "cat /proc/interrupts"
```

The watch command executes another command periodically, in this case `cat /proc/interrupts`

The `-n1` option tells watch to execute the command every second

`-d` option of watch highlight  the  differences  between successive updates

```
$ watch -n 0.1 -d 'cat /proc/interrupts'
```

`--no-title / -t` option of watch Turn off the header showing the interval, command, and current time at the top of the display, as well as the following blank line.

```
$ watch -n 0.1 -d --no-title 'cat /proc/interrupts'
```

### PCI interrutps 

`lspci` is a utility for displaying information about PCI buses in the system and devices connected to them.

By default, it shows a brief list of devices. Use the options described below to request 
either a more verbose output or output intended for parsing by other programs.

reference: https://man7.org/linux/man-pages/man8/lspci.8.html

```
$ lspci | grep -i Ethernet

$ lspci -s 02:01 -v
```

### Interrupt handlers


Interrupt handlers are the responsibility of the driver managing the hardware.

If the device uses interrupts, then driver must register one interrupt handler.

#### Registering an interrupt handler

Header File: <linux/interrupt.h>

```
int request_irq(unsigned int irq,
        irq_handler_t handler,
        unsigned long flags,
        const char *name,
        void *dev);
```

#### Parameters:

```
irq     --> The interrupt number being requested
            For some devices,for example legacy PC devices such as the system timer or keyboard, this value is typically hard-coded.
            For most other devices, it is probed or otherwise determined programmatically and dynamically.

handler   --> function pointer to the actual interrupt handler that services this interrupt.
              invoked whenever the operating system receives the interrupt
              typedef irqreturn_t (*irq_handler_t)(int, void *);

flags     --> bitmask of options related to interrupt management.

name      --> Name to be displayed in /proc/interrupts

dev       --> Used for shared Interrupt Lines
```

#### Return Value:

Success  -->    Returns Zero
Failure  -->    Non-Zero Value

```               
void free_irq(unsigned int irq_no, void *dev);
```

When the interrupt is released, using the `free_irq()` function, you must send the same pointer value (dev) along with the same interrupt number (irq_no).

### Example: keyboard interrupt

- reference: https://stackoverflow.com/questions/33836541/linux-kernel-how-to-capture-a-key-press-and-replace-it-with-another-key
- reference: https://unix.stackexchange.com/questions/545274/how-does-a-keyboard-press-get-processed-in-the-linux-kernel
- reference: https://elixir.bootlin.com/linux/v6.5.7/source/drivers/tty/vt/keyboard.c
- reference: https://elixir.bootlin.com/linux/v6.5.7/source/drivers/tty/vt/defkeymap.map
- reference: https://unix.stackexchange.com/questions/424273/capturing-keypresses-at-kernel-level
- reference: https://github.com/MemoryDealer/linux-keylogger
- reference: https://github.com/jarun/spy/blob/master/spy.c

When a key is pressed, the keyboard controller informs PIC to cause an interrupt.

IRQ #1 is the keyboard interrupt, so when a key is pressed, IRQ 1 is sent to the PIC. 

PIC tells the CPU an interrupt occurred.

When the CPU acknowledges the "interrupt occurred" signal, the PIC chip sends the interrupt number (between 00h and FFh, or 0 and 255 decimal) to the CPU.

For each key pressed on the keyboard, it generates two interrupts (pressed and release).

```
#include <linux/kernel.h>
#include <linux/module.h>
#include <linux/interrupt.h>
#include <linux/delay.h>

MODULE_LICENSE("GPL");

static int irq = 1,  dev = 0xaa, counter = 0;

static irqreturn_t keyboard_handler(int irq, void *dev)
{
        pr_info("Keyboard Counter:%d\n", counter++);
        return IRQ_NONE;
}

/* registering irq */
static int test_interrupt_init(void)
{
        pr_info("%s: In init\n", __func__);
        return request_irq(irq, keyboard_handler, IRQF_SHARED,"my_keyboard_handler", &dev);
}

static void test_interrupt_exit(void)
{
        pr_info("%s: In exit\n", __func__);
        synchronize_irq(irq); /* synchronize interrupt */
        free_irq(irq, &dev);
}

module_init(test_interrupt_init);
module_exit(test_interrupt_exit);
```
You can check the interrupt handler has been installed properly with  `cat /proc/interrupt`, and at IRQ 1 you should see the name "my_keyboard_handler". 

A keyboard generates two scan codes for each key typed on the system, one scan code for press and the other for release.

Release scan code is 128 (80h) plus the press scan code

```
#include <linux/kernel.h>
#include <linux/module.h>
#include <linux/interrupt.h>
#include <linux/delay.h>

MODULE_LICENSE("GPL");

const unsigned char kbdus[128] =    /* these are the scan codes */
{
  0,  27, '1', '2', '3', '4', '5', '6', '7', '8',	/* 9 */
  '9', '0', '-', '=', '\b',	/* Backspace */
  '\t',			/* Tab */
  'q', 'w', 'e', 'r',	/* 19 */
  't', 'y', 'u', 'i', 'o', 'p', '[', ']', '\n',	/* Enter key */
    0,			/* 29   - Control */
  'a', 's', 'd', 'f', 'g', 'h', 'j', 'k', 'l', ';',	/* 39 */
 '\'', '`',   0,		/* Left shift */
 '\\', 'z', 'x', 'c', 'v', 'b', 'n',			/* 49 */
  'm', ',', '.', '/',   0,				/* Right shift */
  '*',
    0,	/* Alt */
  ' ',	/* Space bar */
    0,	/* Caps lock */
    0,	/* 59 - F1 key ... > */
    0,   0,   0,   0,   0,   0,   0,   0,
    0,	/* < ... F10 */
    0,	/* 69 - Num lock*/
    0,	/* Scroll Lock */
    0,	/* Home key */
    0,	/* Up Arrow */
    0,	/* Page Up */
  '-',
    0,	/* Left Arrow */
    0,
    0,	/* Right Arrow */
  '+',
    0,	/* 79 - End key*/
    0,	/* Down Arrow */
    0,	/* Page Down */
    0,	/* Insert Key */
    0,	/* Delete Key */
    0,   0,   0,
    0,	/* F11 Key */
    0,	/* F12 Key */
    0,	/* All other keys are undefined */
};


static int irq = 1,  dev = 0xaa; /* IRQ 1: number for keyboard (i8042) */

#define KBD_DATA_REG        0x60    /* I/O port for keyboard data */
#define KBD_SCANCODE_MASK   0x7f
#define KBD_STATUS_MASK     0x80

static irqreturn_t keyboard_handler(int irq, void *dev)
{
	char scancode;
	scancode = inb(KBD_DATA_REG);
        /* refererence inb: https://linux.die.net/man/2/inb */
        /* This module is not cross-platform (will work only on x86 architecture, because it's using inb() function) */
        /* #define KBD_DATA_REG	0x60  Keyboard data register (R/W)  */
        /* https://elixir.bootlin.com/linux/v6.5.7/source/kernel/debug/kdb/kdb_keyboard.c#L21 */

	pr_info("Character %c %s\n",
			kbdus[scancode & KBD_SCANCODE_MASK],
			scancode & KBD_STATUS_MASK ? "Released" : "Pressed");
        /* It's performing slow I/O operation (I mean pr_info()) in hardware IRQ handler,
           which should be avoided (ideally threaded IRQs should be used)).*/

        return IRQ_NONE;
}

/* registering irq */
static int test_interrupt_init(void)
{
        pr_info("%s: In init\n", __func__);
        return request_irq(irq, keyboard_handler, IRQF_SHARED,"my_keyboard_handler", &dev);
}

static void test_interrupt_exit(void)
{
        pr_info("%s: In exit\n", __func__);
        synchronize_irq(irq); /* synchronize interrupt */
        free_irq(irq, &dev);
}

module_init(test_interrupt_init);
module_exit(test_interrupt_exit);
```

### Example: ethernet interrupt

```
#include <linux/module.h>
#include <linux/init.h>
#include <linux/interrupt.h>

#define SHARED_IRQ 19
static int irq = SHARED_IRQ, my_dev_id, irq_counter = 0;
module_param(irq, int, S_IRUGO);

static irqreturn_t my_interrupt(int irq, void *dev_id)
{
	irq_counter++;
	pr_info("In the ISR: counter = %d\n", irq_counter);
	return IRQ_NONE;	/* we return IRQ_NONE because we are just observing */
}

static int __init my_init(void)
{
	if (request_irq
	    (irq, my_interrupt, IRQF_SHARED, "my_interrupt", &my_dev_id)) {
		pr_info("Failed to reserve irq %d\n", irq);
		return -1;
	}
	pr_info("Successfully loading ISR handler\n");
	return 0;
}

static void __exit my_exit(void)
{
	synchronize_irq(irq);
	free_irq(irq, &my_dev_id);
	pr_info("Successfully unloading,  irq_counter = %d\n", irq_counter);
}

MODULE_LICENSE("GPL");
module_init(my_init);
module_exit(my_exit);
```

### Example: mouse interrupt

```
#include <linux/module.h>
#include <linux/init.h>
#include <linux/interrupt.h>

#define SHARED_IRQ 12
static int irq = SHARED_IRQ, my_dev_id, irq_counter = 0;
module_param(irq, int, S_IRUGO);

static irqreturn_t my_interrupt(int irq, void *dev_id)
{
	irq_counter++;
	pr_info("In the ISR: counter = %d\n", irq_counter);
	return IRQ_NONE;	/* we return IRQ_NONE because we are just observing */
}

static int __init my_init(void)
{
	if (request_irq
	    (irq, my_interrupt, IRQF_SHARED, "my_interrupt", &my_dev_id)) {
		pr_info("Failed to reserve irq %d\n", irq);
		return -1;
	}
	pr_info("Successfully loading ISR handler\n");
	return 0;
}

static void __exit my_exit(void)
{
	synchronize_irq(irq);
	free_irq(irq, &my_dev_id);
	pr_info("Successfully unloading,  irq_counter = %d\n", irq_counter);
}

MODULE_LICENSE("GPL");
module_init(my_init);
module_exit(my_exit);
```

## Threaded IRQs: request_irq()

- reference: https://embetronicx.com/tutorials/linux/device-drivers/threaded-irq-in-linux-kernel/

It is defined at: https://elixir.bootlin.com/linux/v6.5.7/source/include/linux/interrupt.h#L165

```
/**
 * request_irq - Add a handler for an interrupt line
 * @irq:	The interrupt line to allocate
 * @handler:	Function to be called when the IRQ occurs.
 *		Primary handler for threaded interrupts
 *		If NULL, the default primary handler is installed
 * @flags:	Handling flags
 * @name:	Name of the device generating this interrupt
 * @dev:	A cookie passed to the handler function
 *
 * This call allocates an interrupt and establishes a handler; see
 * the documentation for request_threaded_irq() for details.
 */
static inline int __must_check
request_irq(unsigned int irq, irq_handler_t handler, unsigned long flags,
	    const char *name, void *dev)
{
	return request_threaded_irq(irq, handler, NULL, flags, name, dev);
}
```

`void *dev` can be used to share or pass data with the interrupt handle. 

### Return value of interrupt handlers

Interrupt handlers return an `irqreturn_t` value.

 - `IRQ_NONE`            interrupt was not from this device or was not handled
 - `IRQ_HANDLED`         interrupt was handled by this device

### Interrupt flags

The third parameter of `request_irq`, `flags can be either a zero or a bit mask 
of one or more flags defined in <linux/interrupt.>

- reference: https://elixir.bootlin.com/linux/v6.5.8/source/include/linux/interrupt.h#L40

```
/*
 * These flags used only by the kernel as part of the
 * irq handling routines.
 *
 * IRQF_SHARED - allow sharing the irq among several devices
 * IRQF_PROBE_SHARED - set by callers when they expect sharing mismatches to occur
 * IRQF_TIMER - Flag to mark this interrupt as timer interrupt
 * IRQF_PERCPU - Interrupt is per cpu
 * IRQF_NOBALANCING - Flag to exclude this interrupt from irq balancing
 * IRQF_IRQPOLL - Interrupt is used for polling (only the interrupt that is
 *                registered first in a shared interrupt is considered for
 *                performance reasons)
 * IRQF_ONESHOT - Interrupt is not reenabled after the hardirq handler finished.
 *                Used by threaded interrupts which need to keep the
 *                irq line disabled until the threaded handler has been run.
 * IRQF_NO_SUSPEND - Do not disable this IRQ during suspend.  Does not guarantee
 *                   that this interrupt will wake the system from a suspended
 *                   state.  See Documentation/power/suspend-and-interrupts.rst
 * IRQF_FORCE_RESUME - Force enable it on resume even if IRQF_NO_SUSPEND is set
 * IRQF_NO_THREAD - Interrupt cannot be threaded
 * IRQF_EARLY_RESUME - Resume IRQ early during syscore instead of at device
 *                resume time.
 * IRQF_COND_SUSPEND - If the IRQ is shared with a NO_SUSPEND user, execute this
 *                interrupt handler after suspending interrupts. For system
 *                wakeup devices users need to implement wakeup detection in
 *                their interrupt handlers.
 * IRQF_NO_AUTOEN - Don't enable IRQ or NMI automatically when users request it.
 *                Users will enable it explicitly by enable_irq() or enable_nmi()
 *                later.
 * IRQF_NO_DEBUG - Exclude from runnaway detection for IPI and similar handlers,
 *		   depends on IRQF_PERCPU.
 */
#define IRQF_SHARED		0x00000080
#define IRQF_PROBE_SHARED	0x00000100
#define __IRQF_TIMER		0x00000200
#define IRQF_PERCPU		0x00000400
#define IRQF_NOBALANCING	0x00000800
#define IRQF_IRQPOLL		0x00001000
#define IRQF_ONESHOT		0x00002000
#define IRQF_NO_SUSPEND		0x00004000
#define IRQF_FORCE_RESUME	0x00008000
#define IRQF_NO_THREAD		0x00010000
#define IRQF_EARLY_RESUME	0x00020000
#define IRQF_COND_SUSPEND	0x00040000
#define IRQF_NO_AUTOEN		0x00080000
#define IRQF_NO_DEBUG		0x00100000
```

 - `IRQF_SHARED` informs the kernel that the interrupt can be shared with other devices.

If this flag is not set, then if there is already a handler associated with the requested interrupt, 
the request for interrupt will fail. 

`request_irq` return `-EBUSY` which means that the interrupt was already requested by another device driver withou `IRQF_SHARED`. 

On success it returns 0. 

 - `IRQF_NOBALANCING` Flag to exclude this interrupt from irq balancing. The purpouse of IRQ balancing is to distribute hardware interrupts across processors on a
   multiprocessor system in order to increase performance. Setting this flag forbids to set any CPU affinity for the requested interrupt handler. 

### Example of a module that checks all the interrupts that are not being shared

```
#include <linux/interrupt.h>
#include <linux/kernel.h>
#include <linux/module.h>

#define MAX_IRQS 256

static int irqs[MAX_IRQS];
int dev = 1;

static irqreturn_t handler(int irq, void *dev)
{
	//pr_info("irq:%d\n", irq); /*commented because this will be bothersome if a interrupt is continuosly being triggered */
	return IRQ_NONE;            /* we return IRQ_NONE because we are just observing */
}

static int myinit(void)
{
	int ret, i;irqreturn_t

	for (i = 0; i < MAX_IRQS; ++i) {
		ret = request_irq(
			i,
			handler,
			IRQF_SHARED,
			"myirqhandler0",
			&dev
		);
		irqs[i] = ret;
		if (ret == -EBUSY)
			pr_err("request_irq failed irq = %d ret = %d\n", i, ret);
	}
	return 0;
}

static void myexit(void)
{
	int i;

	for (i = 0; i < MAX_IRQS; ++i) {
		if (!irqs[i]) {
			free_irq(i, &dev);
		}
	}
}

module_init(myinit)
module_exit(myexit)
MODULE_LICENSE("GPL");
```

### SMP IRQ affinity & /proc/irq

reference: https://docs.kernel.org/core-api/irq/irq-affinity.html

Starting with the 2.4 kernel, Linux has gained the ability to assign certain IRQs to specific processors (or groups of processors). This is known as **SMP IRQ affinity**.

The interrupt affinity value for a particular IRQ number is stored in the associated `/proc/irq/IRQ_NUMBER/smp_affinity` file, which can be viewed and modified by the root user. 

The value stored in this file is a hexadecimal bit-mask representing all CPU cores in the system
`/proc/irq/irq_number/smp_affinity_list` contains cpu list

Example: 

```
$ cat /proc/irq/1/smp_affinity

00000000,00000000,00000000,00000038
```
like: 
```
0x38 = 0011 1000 . couting from the left, zero indexed, that points to CPUs 3, 4 and 5. 
```

This mean keyboard interrupt can occur in CPU 3, 4, 5


Setting this value to 1, as follows, means that only CPU 0 can service this interrupt:

```
# echo 1 >/proc/irq/1/smp_affinity # 1 will point to cpu zero: 0001 
# cat /proc/irq/1/smp_affinity
1 
```

Commas can be used to delimit smp_affinity values for discrete 32-bit groups. This is required on systems with more than 32 cores. 

`/proc/irq/default_smp_affinity` specifies default affinity mask that applies to all non-active IRQs.

Once IRQ is allocated/activated its affinity bitmask will be set to the default mask. 


### How can a device driver know if the interrupt handler was activated by an interrupt generated by the device it manages?

All devices that offer interrupt support have a status register that can be read in the handling routine to see if the interrupt was or was not generated by the device 

Example:  For 8250 serial port, this status register is IIR - Interrupt Information Register

```
#include <linux/module.h>
#include <linux/init.h>
#include <linux/interrupt.h>
#include <linux/sched.h>
#include <asm/current.h>

static int irq1 = 12, dev_id;
static int irq2 = 1;

static irqreturn_t my_interrupt(int irq, void *dev_id)
{
	pr_info("irq:%d\t dev_id:%02x\n", irq, *(int *)dev_id);
	return IRQ_NONE;	/* we return IRQ_NONE because we are just observing */
}

static int __init my_init(void)
{
	dev_id = 0x1234;
	if (request_irq
	    (irq1, my_interrupt, IRQF_SHARED, "my_interrupt", &dev_id)) {
		pr_info("Failed to reserve irq %d\n", irq1);
		return -1;
	}
	if (request_irq
	    (irq2, my_interrupt, IRQF_SHARED, "my_interrupt", &dev_id)) {
		pr_info("Failed to reserve irq %d\n", irq1);
		return -1;
	}
	pr_info("Successfully loading ISR handler\n");
	return 0;
}

static void __exit my_exit(void)
{
	synchronize_irq(irq1);
	synchronize_irq(irq2);
	free_irq(irq1, &dev_id);
	free_irq(irq2, &dev_id);
}

MODULE_LICENSE("GPL");
module_init(my_init);
module_exit(my_exit);
```

## Enabling or disabling interrupts

The kernel has an API for enabling and disabling interrupts: 

Header file: <linux/irqflags.h> 

```
local_irq_disable(); // disable all interrupts on the current processor
local_irq_enable();  // enable all interrupts on the current processor
```

On x86 the `local_irq_disable()` is a simple `cli` and `local_irq_enable()` is a simple `sti` instrucctions. 

`cli` and `sti` are assembly calls to clear and set interrupt's flags.

### Why do we need to disable interrupts?

Disabling interrupts, you can guarantee that an **interrupt handler will not preempt your current code**.

Disabling interrupts also **disables kernel preemption**.

**Note**: Disabling kernel preemption does not provide protection from concurrent access from another processor.
In this case use **locks** to prevent another processor from accessing shared data simultaneously.

```
#include <linux/module.h>
#include <linux/init.h>
#include <linux/delay.h>
#include <linux/irqflags.h>

static int __init my_init(void)
{
	pr_info("module is loaded on processor:%d\n", smp_processor_id());
	local_irq_disable();
	pr_info("interrupts disabled on processor:%d\n", smp_processor_id());
	mdelay(10000L);
	local_irq_enable();
	return 0;
}

static void __exit my_exit(void)
{
}

MODULE_LICENSE("GPL");
module_init(my_init);
module_exit(my_exit);
```

### Saving interrupt state before disabling interrupts

`local_irq_disable()` routine is dangerous if some of interrupts were already disabled prior to its invocation.

The corresponding call to `local_irq_enable()` unconditionally enables interrupts, despite the fact that they were off to begin with.

`local_irq_save(flags)`; saves the interrupt state on flags and disables interrupt on that processor.

`local_irq_restore(flags)`; restores the previous interrupt state and enables interrupt on that processor. 

```
#include <linux/module.h>
#include <linux/init.h>
#include <linux/delay.h>
#include <linux/irqflags.h>

static int __init my_init(void)
{
	unsigned long flags;
	pr_info("module is loaded on processor:%d\n", smp_processor_id());
	local_irq_save(flags);
	pr_info("flags:%02lx\n", flags);
	mdelay(1000L);
	local_irq_restore(flags);
	return 0;
}

static void __exit my_exit(void)
{
}

MODULE_LICENSE("GPL");
module_init(my_init);
module_exit(my_exit);
```

### Disabling a specific interrupt line

Disabling a specific interrupt line is also known as "masking out an interrupt line" .

Example: you might want to disable delivery of a device’s interrupts before manipulating its state.

```
void disable_irq(unsigned int irq); // Disables a given interrupt line in interrupt controller.
			            // this disables delivery of the given interrupt to all processors in system

void enable_irq(unsigned int irq);
```

Note: `disable_irq` does not return until any executing handler completes. <br>
	Callers are ensured that:
		a) new interrupts will not be delivered on the given line,
		b) any already executing handlers have exited

### disable_irq_nosync

`void disable_irq_nosync(unsigned int irq);`

The function `disable_irq_nosync()` does not wait for current handlers to complete.

`void synchronize_irq(unsigned int irq);`

The function `synchronize_irq()` waits for a specific interrupt handler to exit, if it is executing, before returning.

`synchronize_irq()` spins until no interrupt handler is running for the given IRQ.

### What happens if i call disable_irq twice and enable_irq once?

Calls to these functions nest.

For each call to `disable_irq()` or `disable_irq_nosync()` on a given interrupt line, a corresponding call to enable_irq() is required. 

Only on the last call to `enable_irq()` is the interrupt line actually enabled.

For example, if `disable_irq()` is called twice, the interrupt line is not actually reenabled until the second call to `enable_irq()`.


### What happens if I disable interrupt line shared among multiple interrupt handlers?

Disabling the line disables interrupt delivery for all devices on the line.

Therefore, drivers for newer devices tend not to use these interfaces.

Because PCI devices have to support interrupt line sharing by specification, they should not use these interfaces at all.

Thus, `disable_irq()` and friends are found more often in drivers for older legacy devices, such as the PC parallel port.


### `irqs_disabled()`

The macro `irqs_disabled()`, returns nonzero if the interrupt system on the local processor is disabled.

Header File: <linux/irqflags.h>

```
#include <linux/module.h>
#include <linux/init.h>
#include <linux/delay.h>
#include <linux/irqflags.h>

void is_irq_disabled(void)
{
	if (irqs_disabled())
		pr_info("IRQ Disabled\n");
	else
		pr_info("IRQ Enabled\n");
}

static int __init my_init(void)
{
	pr_info("module is loaded on processor:%d\n", smp_processor_id());
	local_irq_disable();
	is_irq_disabled();
	mdelay(10000L);;
	local_irq_enable();
	is_irq_disabled();
	return 0;
}

static void __exit my_exit(void)
{
}

MODULE_LICENSE("GPL");
module_init(my_init);
module_exit(my_exit);
```

### Process context or user context

 - reference: https://stackoverflow.com/questions/57987140/difference-between-interrupt-context-and-process-context
 - reference: https://stackoverflow.com/questions/47063693/atomic-context-and-process-context-interrupt-context
 - reference: https://www.kernel.org/doc/html/latest/kernel-hacking/hacking.html#user-context
  
Process Context - Regular processes and syscall invocations execute in this context and it can be interrupted by IRQs

One of the most important parts of a process are the executing program code. This code was read in from a executable file and executed within the program's address space. Normal program execution occurs in User-space. When a program executes a system call or triggers an exception, it enters Kernel-space. At this point, the kernel are said to being "executing on behalf of the process" and are in process context. When in process context, the current macro is valid. Upon exiting the kernel, the process resumes execution in User-space, unless a higher-priority process have become runnable In the interim (transition period), in which case the scheduler is invoked to select the higher priority process.

User context is when you are coming in from a system call or other trap: like userspace, you can be preempted by more important tasks and by interrupts. You can sleep, by calling schedule().

**Note**: You are always in user context on module load and unload, and on operations on the block device layer.

In user context, the current pointer (indicating the task we are currently executing) is valid, and in_interrupt() (include/linux/preempt.h) is false.

__**Warning**__: Beware that if you have preemption or softirqs disabled, `in_interrupt()` will return a false positive.

### Interrupt context or atomic context

 - reference: https://stackoverflow.com/questions/57987140/difference-between-interrupt-context-and-process-context
 - reference: https://stackoverflow.com/questions/47063693/atomic-context-and-process-context-interrupt-context
 - reference: https://stackoverflow.com/questions/57987140/difference-between-interrupt-context-and-process-context
   
When executing a interrupt handler, the kernel is in interrupt context

We know process context is the mode of operation the kernel is in while it is executing on behalf of a process.

Eg. Executing a system call.

As interrupt context is not backed with process, you cannot sleep in interrupt context.

If a function sleeps, you cannot use it from your interrupt handler. Examples: kmalloc with GFP_KERNEL, ssleep.

When executing a interrupt handler or bottom half, the kernel is in interrupt context.Recall That process context is the mode of operation the kernel are in while it's executing on behalf of a process-- For example, executing a system call or running a kernel thread. In process context, the current macro points to the associated task. Furthermore, because a process is coupled to the kernel in process context (because the process is connected to the kernel in the same way as the process above), process context can Sleep or otherwise invoke the scheduler.
Interrupt context, on the other hand, was not associated with a process. The current macro isn't relevant (although it points to the interrupted process). Without a backing process (because there is no process background), interrupt context cannot sleep-how would it ever reschedule? (or how to reschedule it again?) Therefore, cannot call certain functions from interrupt context. If a function sleeps, you cannot use it from your interrupt handler--this limits the functions so one can call from an Interrupt handler. (This is the limit on what functions can be used in an interrupt handler).

IRQs are generally executed in this context and they don't belong to any specific process, but rather are invoked by some device(ignore exceptions for simplicity). Once the interrupt context sleeps or gives up the CPU, it cannot be awakened. So it is also called atomic context.

A basic principle of the kernel is that in an interrupt or atomic context, the kernel cannot access user space, and the kernel cannot sleep.


### How to find out if we are in an interrupt context?

Using the function `in_interrupt()` 

Header File: <linux/preempt.h>

To find out whether you are running in interrupt context or process context:
 - `in_interrupt()` returns non zero if the kernel is performing any type of interrupt handling.
 - `in_interrupt()` returns zero if the kernel is in process context.

You can use that macro to know if you can or not allocate memory, for example: 

```
#include <linux/module.h>
#include <linux/init.h>
#include <linux/interrupt.h>
#include <linux/slab.h>


#define SHARED_IRQ 12
static int irq = SHARED_IRQ, my_dev_id, irq_counter = 0;
module_param(irq, int, S_IRUGO);

void *alloc_mem(unsigned int size)
{
	// depending on the value of in_Interrupt()
	// we can allocate memory or not.
	if (in_interrupt()) {
		return kmalloc(size, GFP_ATOMIC);  // iGFP_ATOMIC n case of interrupt context
	} else {
		return kmalloc(size, GFP_KERNEL);  // GFP_KERNEL in case of process context
	}
}

static irqreturn_t my_interrupt(int irq, void *dev_id)
{
	void *mem = alloc_mem(1024);
	kfree(mem);
	return IRQ_NONE;	/* we return IRQ_NONE because we are just observing */
}

static int __init my_init(void)
{
	void *mem = alloc_mem(1024);
	kfree(mem);
	if (request_irq
	    (irq, my_interrupt, IRQF_SHARED, "my_interrupt", &my_dev_id)) {
		pr_info("Failed to reserve irq %d\n", irq);
		return -1;
	}
	pr_info("Successfully loading ISR handler\n");
	return 0;
}

static void __exit my_exit(void)
{
	synchronize_irq(irq);
	free_irq(irq, &my_dev_id);
	pr_info("Successfully unloading,  irq_counter = %d\n", irq_counter);
}

MODULE_LICENSE("GPL");
module_init(my_init);
module_exit(my_exit);
```

### Some debuging: printing the call stack 

Use the function `dump_stack();`:

```
#include <linux/module.h>
#include <linux/init.h>
#include <linux/interrupt.h>

#define SHARED_IRQ 12
static int irq = SHARED_IRQ, my_dev_id, irq_counter = 0;
module_param(irq, int, S_IRUGO);

static irqreturn_t my_interrupt(int irq, void *dev_id)
{
	dump_stack();
	return IRQ_NONE;	/* we return IRQ_NONE because we are just observing */
}

static int __init my_init(void)
{
	if (request_irq
	    (irq, my_interrupt, IRQF_SHARED, "my_interrupt", &my_dev_id)) {
		pr_info("Failed to reserve irq %d\n", irq);
		return -1;
	}
	pr_info("Successfully loading ISR handler\n");
	return 0;
}

static void __exit my_exit(void)
{
	synchronize_irq(irq);
	free_irq(irq, &my_dev_id);
	pr_info("Successfully unloading,  irq_counter = %d\n", irq_counter);
}

MODULE_LICENSE("GPL");
module_init(my_init);
module_exit(my_exit);
```

### Can we use current macro inside interrupt handler?

It will point to the interrupted process.

```
#include <linux/module.h>
#include <linux/init.h>
#include <linux/interrupt.h>
#include <linux/sched.h>
#include <asm/current.h>

#define SHARED_IRQ 12
static int irq = SHARED_IRQ, my_dev_id, irq_counter = 0;
module_param(irq, int, S_IRUGO);

static irqreturn_t my_interrupt(int irq, void *dev_id)
{
	irq_counter++;
	pr_info("In the ISR: counter = %d\n", irq_counter);
	pr_info("current pid : %d , current process : %s\n",current->pid, current->comm);
	return IRQ_NONE;	/* we return IRQ_NONE because we are just observing */
}

static int __init my_init(void)
{
	if (request_irq
	    (irq, my_interrupt, IRQF_SHARED, "my_interrupt", &my_dev_id)) {
		pr_info("Failed to reserve irq %d\n", irq);
		return -1;
	}
	pr_info("Successfully loading ISR handler\n");
	return 0;
}

static void __exit my_exit(void)
{
	synchronize_irq(irq);
	free_irq(irq, &my_dev_id);
	pr_info("Successfully unloading,  irq_counter = %d\n", irq_counter);
}

MODULE_LICENSE("GPL");
module_init(my_init);
module_exit(my_exit);
```

### What happens if we `sleep()` in the interrupt handler?

It will crash :/
	

## Threaded interrupts: Top Half and Bottom Half


Two important goals of interrupt handler are:

#### Execution Time

Handler of an interrupt must execute quickly. Long running handlers can slow down the system and may also lead to losing interrupts.
The faster the handler returns, the lower the interrupt latencies in the kernel, which is especially important for real-time systems

#### Execution Context

Interrupt handlers are executed in hard-interrupt context – CPU-local interrupts remain disabled. So Locking is undesirable and sleeping must be avoided.
Large amount of work cannot be performed in interrupt handler, exection times must be kept low.


Both limitations lead to the fact that most interrupt handlers execute only a small amount of code and defer the rest of the work to a later point in time

Handling of interrupts is divided into two parts:

1.- Top Half (Hard IRQ)
    - Acknowledge the interrupt
    - Copy the necessary stuff from the device
    - Schedule the bottom half

2. Bottom Half (Soft IRQ)
    - Remaining pending work

###  Top and bottom halves with an example

Network Card on reception of packet from the network, issues an interrupt, kernel responds it by executing the handler

Top Half(Interrupt Handler): 
	- Acknowledges the interrupt, 
	- Copies the new networking packets into main memory,
	- Pushes it up to the protocol layer
	- Readies the network card for more packets.
	- Schedule the bottom half

Bottom Half: 
        - Rest of the processing and handling of the packets


Various Mechanisms available for Bottom Half
 - Soft IRQ
 - Tasklets
 - Workqueue
 - Timers

   
### Threaded IRQs

- reference: https://embetronicx.com/tutorials/linux/device-drivers/threaded-irq-in-linux-kernel/

An alternative to using formal bottom-half mechanisms is threaded interrupt handlers.

Threaded interrupt handlers seeks to reduce the time spent with interrupts disabled to bare minimum, pushing the rest of the processing out into kernel threads.

With threaded IRQs, the way you register an interrupt handler is a bit simplified.

You do not even have to schedule the bottom half yourself. The core does that for us.

The bottom half is then executed in a dedicated kernel thread. 

```
int request_threaded_irq (unsigned int irq,
		irq_handler_t handler,
		irq_handler_t thread_fn,
		unsigned long irqflags,
		const char * devname,
		void * dev_id);
```

#### Difference between request_irq and request_threaded_irq

```
irq_handler_t thread_fn

request_threaded_irq() breaks handler code in two parts, 
	handler and 
	thread function
```

Now, the main functionality of the handler is to intimate hardware that it has received the interrupt and wake up thread function

As soon as handler finishes, processor is in process context. 
```
kernel/irq/manage.c --- setup_irq_thread
```
priority of the thread is set to `MAX_USER_RT_PRIO/2` which is higher than regular processes

Example: 
```
#include <linux/module.h>
#include <linux/init.h>
#include <linux/interrupt.h>
#include <linux/delay.h>

#define SHARED_IRQ 19
static int irq = SHARED_IRQ, my_dev_id, irq_counter = 0;
module_param(irq, int, S_IRUGO);

static irqreturn_t my_interrupt(int irq, void *dev_id)
{
	pr_info("%s\n", __func__);
	return IRQ_NONE;
}

static irqreturn_t my_threaded_interrupt(int irq, void *dev_id)
{
	pr_info("%s\n", __func__);
	return IRQ_NONE;
}

static int __init my_init(void)
{
	int ret;
	ret =  (request_threaded_irq(irq, 
			my_interrupt, my_threaded_interrupt,
			IRQF_SHARED, "my_interrupt", &my_dev_id)); 
	
	if (ret != 0)
	{
	
		pr_info("Failed to reserve irq %d, ret:%d\n", irq, ret);
		return -1;
	}
	pr_info("Successfully loading ISR handler\n");
	return 0;
}

static void __exit my_exit(void)
{
	synchronize_irq(irq);
	free_irq(irq, &my_dev_id);
	pr_info("Successfully unloading,  irq_counter = %d\n", irq_counter);
}

MODULE_LICENSE("GPL");
module_init(my_init);
module_exit(my_exit);
```

#### Why is the threaded handler not being executed even after thread is created?

When the hard-IRQ handler (handler function) function returns IRQ_WAKE_THREAD, 

the kthread associated with this bottom half will be scheduled, invoking the thread_fn

The thread_fn function must return IRQ_HANDLED when complete.

After being executed, the kthread will not be rescheduled again until the IRQ is triggered again and the hard-IRQ returns IRQ_WAKE_THREAD

So the example should be:

```
#include <linux/module.h>
#include <linux/init.h>
#include <linux/interrupt.h>
#include <linux/delay.h>

#define SHARED_IRQ 19
static int irq = SHARED_IRQ, my_dev_id, irq_counter = 0;
module_param(irq, int, S_IRUGO);

void print_context(void)
{
        if (in_interrupt()) {
                pr_info("Code is running in interrupt context\n");
        } else {
                pr_info("Code is running in process context\n");
        }

}


static irqreturn_t my_interrupt(int irq, void *dev_id)
{
	pr_info("%s\n", __func__);
	print_context();
	return IRQ_WAKE_THREAD;
}

static irqreturn_t my_threaded_interrupt(int irq, void *dev_id)
{
	pr_info("%s\n", __func__);
	print_context();
	return IRQ_NONE;
}

static int __init my_init(void)
{
	int ret;
	ret =  (request_threaded_irq(irq, 
			my_interrupt, my_threaded_interrupt,
			IRQF_SHARED, "my_interrupt", &my_dev_id)); 
	
	if (ret != 0)
	{
	
		pr_info("Failed to reserve irq %d, ret:%d\n", irq, ret);
		return -1;
	}
	pr_info("Successfully loading ISR handler\n");
	return 0;
}

static void __exit my_exit(void)
{
	synchronize_irq(irq);
	free_irq(irq, &my_dev_id);
	pr_info("Successfully unloading,  irq_counter = %d\n", irq_counter);
}
```

to identify the threads that are kthread belonging to a threaded handler, use:

```
ps -ef | grep irq/
```


#### IRQF_ONESHOT

The interrupt is not reenabled after the IRQ handler finishes.

This flag is required for threaded interrupts which need to keep the interrupt line disabled until the threaded handler has run.

Specifying this flag is mandatory if the primary handler is set to NULL.

The default primary handler does nothing more than to return IRQ WAKE THREAD to wake up a kernel thread to execute the thread fn IRQ handler.

`kernel/irq/manage.c 	-->	irq_default_primary_handler`


```
#include <linux/kernel.h>
#include <linux/module.h>
#include <linux/gpio.h>
#include <linux/delay.h>
#include <linux/interrupt.h>

#define GPIO_BASE               0x3f200000      // GPIO controller 
#define GPIO_SIZE               0xb4

#define GPFSEL0_OFFSET          0x00
#define GPSET0_OFFSET           0x07
#define GPCLR0_OFFSET           0x0A
#define GPPUD_OFFSET		0x25
#define GPPUDCLK0_OFFSET	0x26

static unsigned int irq_number;
static unsigned int gpio_button = 15;

MODULE_LICENSE("GPL");
uint32_t *mem;

void set_gpio_pulldown(unsigned int gpio)
{
	int register_index = gpio/32;
	unsigned int value = (1 << (gpio % 32));

	mem = (uint32_t *)ioremap(GPIO_BASE, GPIO_SIZE);
	iowrite32(0x01, mem + GPPUD_OFFSET); //enable pull down
	// Wait 150 cycles
	udelay(2000);
	//Write to GPPUDCLK0/1 to clock the control signal into the GPIO pads you wish to modify
	iowrite32(value, mem + GPPUDCLK0_OFFSET + register_index);	
	// Wait 150 cycles
	udelay(2000);
	//Write to GPPUD to remove the control signal
	iowrite32(0x00, mem + GPPUD_OFFSET);
	//Write to GPPUDCLK0/1 to remove the clock
	iowrite32(0x00, mem + GPPUDCLK0_OFFSET + register_index);	

	iounmap(mem);
}

static irqreturn_t  button_threaded_handler(int irq, void *dev_id)
{
        pr_info("threaded irq:%d\n", irq);
	mdelay(4000);
        return IRQ_HANDLED;
}

static int test_hello_init(void)
{
	pr_info("%s: In init\n", __func__);

	if (!gpio_is_valid(gpio_button)){
		pr_info("Invalid GPIO:%d\n", gpio_button);
		return -ENODEV;
	}

	pr_info("gpio button:%d is valid\n", gpio_button);
	if (gpio_request(gpio_button, "my_button")) {
		pr_info("GPIO Request Failed on gpio:%d\n", gpio_button);
		return -EINVAL;
	}
	pr_info("GPIO Request successful on gpio:%d\n", gpio_button);
	gpio_direction_output(gpio_button, 0);
	gpio_direction_input(gpio_button);
	gpio_set_debounce(gpio_button, 1000);      // Debounce the button with a delay of 1000ms
	set_gpio_pulldown(gpio_button);
	irq_number = gpio_to_irq(gpio_button);
        pr_info("irq number:%d\n", irq_number);

	return request_threaded_irq(irq_number, NULL, button_threaded_handler,
			IRQF_TRIGGER_FALLING | IRQF_ONESHOT, 
			"button_interrupt",
			NULL);
}

static void test_hello_exit(void)
{
	pr_info("%s: In exit\n", __func__);
	free_irq(irq_number, NULL);
	gpio_free(gpio_button);
}

module_init(test_hello_init);
module_exit(test_hello_exit);
```

#### debuging: printing call trace in threaded irqs

Use the function `dump_stack()`

```
#include <linux/module.h>
#include <linux/init.h>
#include <linux/interrupt.h>
#include <linux/delay.h>

#define SHARED_IRQ 19
static int irq = SHARED_IRQ, my_dev_id, irq_counter = 0;
module_param(irq, int, S_IRUGO);

static irqreturn_t my_interrupt(int irq, void *dev_id)
{
	pr_info("%s\n", __func__);
	return IRQ_WAKE_THREAD;
}

static irqreturn_t my_threaded_interrupt(int irq, void *dev_id)
{
	dump_stack();
	return IRQ_NONE;
}

static int __init my_init(void)
{
	int ret;
	ret =  (request_threaded_irq(irq, 
			my_interrupt, my_threaded_interrupt,
			IRQF_SHARED, "my_interrupt", &my_dev_id)); 
	
	if (ret != 0)
	{
	
		pr_info("Failed to reserve irq %d, ret:%d\n", irq, ret);
		return -1;
	}
	pr_info("Successfully loading ISR handler\n");
	return 0;
}

static void __exit my_exit(void)
{
	synchronize_irq(irq);
	free_irq(irq, &my_dev_id);
	pr_info("Successfully unloading,  irq_counter = %d\n", irq_counter);
}

MODULE_LICENSE("GPL");
module_init(my_init);
module_exit(my_exit);
```

### Printing PID and process name in threaded irqs.

Use the macro current.

```
#include <linux/module.h>
#include <linux/init.h>
#include <linux/interrupt.h>
#include <linux/delay.h>
#include <linux/sched.h>

#define SHARED_IRQ 19
static int irq = SHARED_IRQ, my_dev_id, irq_counter = 0;
module_param(irq, int, S_IRUGO);

static irqreturn_t my_interrupt(int irq, void *dev_id)
{
	pr_info("%s\n", __func__);
	return IRQ_WAKE_THREAD;
}

static irqreturn_t my_threaded_interrupt(int irq, void *dev_id)
{
	pr_info("COMM:%s\t PID:%d\n", current->comm, current->pid);
	return IRQ_NONE;
}

static int __init my_init(void)
{
	int ret;
	ret =  (request_threaded_irq(irq, 
			my_interrupt, my_threaded_interrupt,
			IRQF_SHARED, "my_interrupt", &my_dev_id)); 
	
	if (ret != 0)
	{
	
		pr_info("Failed to reserve irq %d, ret:%d\n", irq, ret);
		return -1;
	}
	pr_info("Successfully loading ISR handler\n");
	return 0;
}

static void __exit my_exit(void)
{
	synchronize_irq(irq);
	free_irq(irq, &my_dev_id);
	pr_info("Successfully unloading,  irq_counter = %d\n", irq_counter);
}

MODULE_LICENSE("GPL");
module_init(my_init);
module_exit(my_exit);
```

#### How to check that the interrupts are enable or disabled:

```
#include <linux/module.h>
#include <linux/init.h>
#include <linux/interrupt.h>
#include <linux/delay.h>
#include <linux/sched.h>
#include <linux/irqflags.h>

#define SHARED_IRQ 19
static int irq = SHARED_IRQ, my_dev_id, irq_counter = 0;
module_param(irq, int, S_IRUGO);

void is_irq_disabled(void)
{
        if (irqs_disabled())
                pr_info("IRQ Disabled\n");
        else
                pr_info("IRQ Enabled\n");
}


static irqreturn_t my_interrupt(int irq, void *dev_id)
{
	is_irq_disabled();
	return IRQ_WAKE_THREAD;
}

static irqreturn_t my_threaded_interrupt(int irq, void *dev_id)
{
	pr_info("COMM:%s\t PID:%d\n", current->comm, current->pid);
	is_irq_disabled();
	return IRQ_NONE;
}

static int __init my_init(void)
{
	int ret;
	ret =  (request_threaded_irq(irq, 
			my_interrupt, my_threaded_interrupt,
			IRQF_SHARED, "my_interrupt", &my_dev_id)); 
	
	if (ret != 0)
	{
	
		pr_info("Failed to reserve irq %d, ret:%d\n", irq, ret);
		return -1;
	}
	pr_info("Successfully loading ISR handler\n");
	return 0;
}

static void __exit my_exit(void)
{
	synchronize_irq(irq);
	free_irq(irq, &my_dev_id);
	pr_info("Successfully unloading,  irq_counter = %d\n", irq_counter);
}

MODULE_LICENSE("GPL");
module_init(my_init);
module_exit(my_exit);
```


## SoftIrqs

references: 
 - https://www.kernel.org/doc/html/latest/kernel-hacking/hacking.html#software-interrupt-context-softirqs-and-tasklets
 - https://www.oreilly.com/library/view/understanding-the-linux/0596005652/ch04s07.html
 - https://linux-kernel-labs.github.io/refs/pull/189/merge/labs/deferred_work.html#softirqs
 - https://embetronicx.com/tutorials/linux/device-drivers/softirq-in-linux-kernel/
 - https://elixir.bootlin.com/linux/v6.6.8/source/include/linux/interrupt.h#L548
 - https://elixir.bootlin.com/linux/v6.6.8/source/kernel/softirq.c#L63
 - https://blog.netdata.cloud/understanding-interrupts-softirqs-and-softnet-in-linux/


What are Softirqs?
Softirqs (software interrupt requests) are similar to interrupts, but they are generated by the kernel itself rather than hardware devices. Softirqs are used for a variety of purposes, such as network processing, task scheduling, and disk I/O. Softirqs are generally less time-critical than hardware interrupts, but they can still impact system performance.

Here is a list of the possible softirqs:

 - TIMER: This softirq is used for timer management, such as scheduling periodic tasks or triggering events after a certain amount of time has elapsed.
 - NET_TX: This softirq is used for network packet transmission, such as sending data over a network interface card.
 - NET_RX: This softirq is used for network packet reception, such as processing incoming data from a network interface card.
 - TASKLET: This softirq is used for task scheduling, such as scheduling work to be done in response to an event or interrupt.
 - SCHED: This softirq is used for task scheduling as well, but it is used for higher-priority scheduling activities.
 - HRTIMER: This softirq is used for high-resolution timer management, such as scheduling tasks with precise timing requirements.
 - RCU (Read-Copy Update): This softirq is used for synchronization between multiple threads or processes that access shared data. It ensures that shared data is not modified by one thread while another thread is reading it.
 - BLOCK: This softirq is used for block device I/O operations, such as reading from or writing to a hard disk.
 - IRQ_POLL: This softirq is used to poll hardware devices for new data, rather than waiting for an interrupt.
 - IRQ_THREADED: This softirq is used for threaded interrupt handling, which allows interrupt handling to be offloaded to a separate thread rather than being handled directly by the kernel.
 - SCHEDSTAT: This softirq is used for collecting scheduling statistics, such as the number of processes in each scheduling class.
 - SLOWPATH: This softirq is used for slow-path packet processing, such as handling packets that require additional processing or validation.


### Softirqs 


Softirqs are bottom halves that run at a high priority but with hardware interrupts enabled

Implementation: kernel/softirq.c

Header File: <linux/softirq.h>

### Data structures

Softirqs are represented by the softirq_action structure, which is a function pointer. 

```
struct softirq_action
{
        void    (*action)(struct softirq_action *);
};
```

A 10 entry array of this structure is declared in kernel/softirq.c:  - https://elixir.bootlin.com/linux/v6.6.8/source/kernel/softirq.c#L63

```
const char * const softirq_to_name[NR_SOFTIRQS] = {
	"HI", "TIMER", "NET_TX", "NET_RX", "BLOCK", "IRQ_POLL",
	"TASKLET", "SCHED", "HRTIMER", "RCU"
};
```

static struct softirq_action softirq_vec[NR_SOFTIRQS];
 - two for tasklet processing (HI_SOFTIRQ and TASKLET_SOFTIRQ),
 - two for send and receive operations in networking (NET_TX_SOFTIRQ and NET_RX_SOFTIRQ),
 - two for the block layer (asynchronous request completions),
 - two for timers, and 
 - one each for the scheduler and 
 - read-copy-update processing

From include/linux/interrupt.h: - https://elixir.bootlin.com/linux/v6.6.8/source/include/linux/interrupt.h#L548

```
enum
{
	HI_SOFTIRQ=0,
	TIMER_SOFTIRQ,
	NET_TX_SOFTIRQ,
	NET_RX_SOFTIRQ,
	BLOCK_SOFTIRQ,
	IRQ_POLL_SOFTIRQ,
	TASKLET_SOFTIRQ,
	SCHED_SOFTIRQ,
	HRTIMER_SOFTIRQ,
	RCU_SOFTIRQ,    /* Preferable RCU should always be the last softirq */

	NR_SOFTIRQS
};
```

The number of registered softirqs is statically determined and cannot be changed dynamically.


### Preeemption

A softirq never preempts another softirq . The only event that can preempt a softirq is interrupt handler.

Another softirq even the same one can run on another processor.

### statistics of softirqs

In the file: /proc/softirqs

Shows Per CPU statistics

Implementation: fs/proc/softirqs.c

```
$ cat /proc/softirq
$ watch -d -n 0.1 cat /proc/softirqs
$ watch -n1 grep RX /proc/softirqs
```

### softirq methods


### Registering softirq handlers:


Software interrupts must be registered before the kernel can execute them.

open_softirq is used for associating the softirq instance with the corresponding bottom halve routine.

```
void open_softirq(int nr, void (*action)(struct softirq_action *))
{
        softirq_vec[nr].action = action;
}
```

It is being called for example from networking subsystem.

net/core/dev.c:
```
        open_softirq(NET_TX_SOFTIRQ, net_tx_action);
        open_softirq(NET_RX_SOFTIRQ, net_rx_action);
```

### Execution of softirq:

The kernel maintains a per-CPU bitmask indicating which softirqs need processing at any given time
`irq_stat[smp_processor_id].__softirq_pending`.

Drivers can signal the execution of soft irq handlers using a function raise_softirq().
This function takes the index of the softirq as argument.

```
void raise_softirq(unsigned int nr)
{
        unsigned long flags;

        local_irq_save(flags);
        raise_softirq_irqoff(nr);
        local_irq_restore(flags);
}
```

```
local_irq_save 		--> Disables interrupts on the current processor where code is running
raise_softirq_irqoff 	--> sets the corresponding bit in the local CPUs softirq bitmask to mark the specified softirq as pending
local_irq_restore	--> Enables the interrupts
```

`raise_softirq_irqoff` if executed in non-interrupt context, will invoke `wakeup_softirqd()`, to wake up, if necessary the ksoftirqd kernel thread of that local CPU

### What is the benefit of per-CPU Bitmask?


By using a processors specific bitmap, the kernel ensures that several softIRQs — even identical ones — can be executed on different CPUs at the same time.


### Executing Softirqs

The actual execution of softirqs is managed by do_softirq()

Implementation : kernel/softirq.c

`do_softirq()` will call __do_softirq(), if any bit in the local softirq bit mask is set.

`__do_softirq()` then iterates over the softirq bit mask (least signicant bit) and invokes scheduled softirq handlers.


### Creating a new softirq

An example for creating a new softirq on a raspberry pi is described next:



You declare softirqs statically at compile time via an enum in <linux/interrupt.h>.

Creating a new softirq includes adding a new entry to this enum.

The index is used by the kernel as priority.

Softirqs with the lowest numerical priority execute before those with a higher numerical priority

Insert the new entry depending on the priority you want to give it.

- https://elixir.bootlin.com/linux/v6.6.8/source/include/linux/interrupt.h#L548

```
enum
{
	HI_SOFTIRQ=0,
	TIMER_SOFTIRQ,
	NET_TX_SOFTIRQ,
	NET_RX_SOFTIRQ,
	BLOCK_SOFTIRQ,
	IRQ_POLL_SOFTIRQ,
	TASKLET_SOFTIRQ,
	SCHED_SOFTIRQ,
	HRTIMER_SOFTIRQ,
        MY_SOFIRQ,
	RCU_SOFTIRQ,    /* Preferable RCU should always be the last softirq */

	NR_SOFTIRQS
};
```

- https://elixir.bootlin.com/linux/v6.6.8/source/kernel/softirq.c#L63

```
const char * const softirq_to_name[NR_SOFTIRQS] = {
	"HI", "TIMER", "NET_TX", "NET_RX", "BLOCK", "IRQ_POLL",
	"TASKLET", "SCHED", "HRTIMER", "MY_SOFTIRQ", "RCU"
};
```

### Create the softirq handler

Define it, for example:

```
void my_action(struct softirq_action *h)
{
        pr_info("my_action\n");
}
```

### Registering your handler


Soft irq is registered at runtime via open_softirq().

```
open_softirq(MY_SOFTIRQ, my_action);
```

It takes two parameters:
 - Index: Softirq entry. In our case, MY_SOFTIRQ
 - Handler Function: Softirq handler, in this case my_action

```
 	open_softirq(MY_SOFTIRQ, my_action);
```

### Raising your softirq

To mark it pending, so it is run at the next invocation of `do_softirq()`, call `raise_softirq()`.

Softirqs are most often raised from within interrupt handlers.

```
static irqreturn_t  button_handler(int irq, void *dev_id)
{
        pr_info("irq:%d\n", irq);
	raise_softirq(MY_SOFTIRQ);
        return IRQ_HANDLED;
}
```

### Other Details

The softirq handlers run with interrupts enabled and cannot sleep.

While a handler runs, softirqs on the current processor are disabled.

Another processor, can however execute another softirq.

If the same softirq is raised again while it is executing, another processor can run in it simultaneously.

This means that any shared data even global data used only within the soft irq handler needs proper locking.

most softirq handlers resort to per-processor data (data unique to each processor and thus not requiring locking) and other tricks to avoid explicit locking and provide excellent scalability.


To compile it, the whole Linux kernel has to be compiled: 

so this file is moved into the rpi `cp ./hello.c ~/raspberrypi/linux/drivers/misc`

```
cp ./hello.c ~/raspberrypi/linux/drivers/misc
cd drivers/misc
vi Makefile
# add: obj-y += hello.o // this will compile the file in the kernel image
cd ../..
# now compile the kernel
make ARCH=arm CROSS_COMPILE=arm-linux-gnueabihf- -j$(nproc)
# move the image to the rpi
scp arch/arm/boot/zImage pi@192.168.0.102:/home/pi/kernel2.img
# now we go into the rpi
sudo cp kernel2.img /boot/
sync
sudo reboot 
```

you can check the new soft irq with `cat /proc/interrupts`. or `cat /proc/softirqs`



### Example for new rasbperry pi soft irq code

the code 

```
#include <linux/kernel.h>
#include <linux/module.h>
#include <linux/gpio.h>
#include <linux/delay.h>
#include <linux/interrupt.h>

#define GPIO_BASE               0x3f200000      // GPIO controller 
#define GPIO_SIZE               0xb4

#define GPFSEL0_OFFSET          0x00
#define GPSET0_OFFSET           0x07
#define GPCLR0_OFFSET           0x0A
#define GPPUD_OFFSET		0x25
#define GPPUDCLK0_OFFSET	0x26

static unsigned int irq_number;
static unsigned int gpio_button = 15;

MODULE_LICENSE("GPL");
uint32_t *mem;

void set_gpio_pulldown(unsigned int gpio)
{
	int register_index = gpio/32;
	unsigned int value = (1 << (gpio % 32));

	mem = (uint32_t *)ioremap(GPIO_BASE, GPIO_SIZE);
	iowrite32(0x01, mem + GPPUD_OFFSET); //enable pull down
	// Wait 150 cycles
	udelay(2000);
	//Write to GPPUDCLK0/1 to clock the control signal into the GPIO pads you wish to modify
	iowrite32(value, mem + GPPUDCLK0_OFFSET + register_index);	
	// Wait 150 cycles
	udelay(2000);
	//Write to GPPUD to remove the control signal
	iowrite32(0x00, mem + GPPUD_OFFSET);
	//Write to GPPUDCLK0/1 to remove the clock
	iowrite32(0x00, mem + GPPUDCLK0_OFFSET + register_index);	
	
	iounmap(mem);
}

// <<<<< this function will report the context in which the softirq is running 
void print_context(void)  
{

        if (in_interrupt()) {
                pr_info("Code is running in interrupt context\n");
        } else {
                pr_info("Code is running in process context\n");
        }
}

// <<<< this function will report if the irq are enabled or disabled.
void is_irq_disabled(void)
{
        if (irqs_disabled())
                pr_info("IRQ Disabled\n");
        else
                pr_info("IRQ Enabled\n");
}

// <<<< print the context
void print_context(void)
{
        if (in_irq()) {
                pr_info("Code is running in hard irq context\n");
        } else {
                pr_info("Code is NOT running in hard irq context\n");
        }

        if (in_softirq()) {
                pr_info("Code is running in soft irq context\n");
        } else {
                pr_info("Code is NOT running in soft irq context\n");
        }
}

// <<<< this is the softirq handler
void my_action(struct softirq_action *h)
{
        pr_info("my_action\n");
        print_context();                // this will print the context, being a softirq, this should be an interrupt context
        irq_disabled();                 // here, in the softirq handler, the IRQs should be enabled. So the interrupt handler, can preempt tine softirq handler. Whereas hardirq run with interrupts disabled. 
        pr_info("current pid : %d , current process : %s\n",current->pid, current->comm);  // the process here should be `swapper`that is the interrupt process.
        pr_info("my_action processor id:%d\n", smp_processor_id());                        // info about the processor id
        print_context();               // the interrupt handler should run in soft interrupt context (irq enabled)
        dump_stack();                  // in the stack should be the function __do_softirq()
}

static irqreturn_t button_handler(int irq, void *dev_id)
{
        pr_info("irq:%d\n", irq);
        print_context();               // this will print the context, being a softirq, this should be an interrupt context
	raise_softirq(MY_SOFTIRQ);     // <<<< here the new softirq is raised
                                       // this will mark the softirq as pending
        irq_disabled();                // here, in the inerrupt handler, the IRQs should be disabled.
        pr_info("current pid : %d , current process : %s\n",current->pid, current->comm); // the process here should be `swapper`that is the interrupt process.
        pr_info("my_action processor id:%d\n", smp_processor_id());                        // info about the processor id
        print_context();               // the interrupt handler should run in hard interrupt context (irq disabled)
        dump_stack();
        return IRQ_HANDLED;
}

static int test_hello_init(void)
{
	pr_info("%s: In init\n", __func__);

	if (!gpio_is_valid(gpio_button)){
		pr_info("Invalid GPIO:%d\n", gpio_button);
		return -ENODEV;
	}

	pr_info("gpio button:%d is valid\n", gpio_button);
	if (gpio_request(gpio_button, "my_button")) {
		pr_info("GPIO Request Failed on gpio:%d\n", gpio_button);
		return -EINVAL;
	}
	pr_info("GPIO Request successful on gpio:%d\n", gpio_button);

	gpio_direction_input(gpio_button);
	gpio_set_debounce(gpio_button, 1000);      // Debounce the button with a delay of 1000ms
	set_gpio_pulldown(gpio_button);
	irq_number = gpio_to_irq(gpio_button);
        pr_info("irq number:%d\n", irq_number);
	open_softirq(MY_SOFTIRQ, my_action);       // <<<<< here the handler to the new softirq is registered
	return request_irq(irq_number, button_handler,
			IRQF_TRIGGER_FALLING,
			"button_interrupt",
			NULL);
}

static void test_hello_exit(void)
{
	pr_info("%s: In exit\n", __func__);
	free_irq(irq_number, NULL);
	gpio_free(gpio_button);
}

module_init(test_hello_init);
module_exit(test_hello_exit);
```

the makefile

```
obj-m := hello.o

all:
        make ARCH=arm CROSS_COMPILE=arm-linux-gnueabihf- -C /home/linuxtrainer/raspberrypi/linux M=$(PWD) modules
clean:
        make ARCH=arm CROSS_COMPILE=arm-linux-gnueabihf- -C /home/linuxtrainer/raspberrypi/linux M=$(PWD) clean
```


### Important Points related to softirqs

1. Compile Time:
  - Declared at compile time in an enumerator.
  - Not suitable for linux kernel modules.

2. Execution:
  - Executed as early as possible.
     - After return of a top handler and before return to a system call.
  - This is achieved by giving a high priority to the executed softirq handlers.

3. Parallel:
 - Softirqs can run in parallel.
 - Each processor has its own softirq bitmap.
 - One softirq cannot be scheduled twice on the same processor.
 - One softirq may run in parallel on other.

4. Priority:
 - Kernel iterates over the softirq bitmap, least significant bit (LSB) first, and execute the associated softirq handlers.
	

### ksoftirqd

- reference: https://www.oreilly.com/library/view/linux-device-drivers/9781785280009/4924e580-97c8-4b45-8226-c900a49e27e0.xhtml

Softirqs are executed as long as the processor-local softirq bitmap is set.

Since softirqs are bottom halves and thus remain interruptible during execution, 

the system can find itself in a state where it does nothing else than 
	serving interrupts and 
	softirqs

incoming interrupts may schedule softirqs what leads to another iteration over the bitmap.

Such processor-time monopolization by softirqs is acceptable under high workloads (e.g., high IO or network traffic), but it is generally undesirable for a longer period of time since (user) processes cannot be executed.

### how to solfe the processor-time monopilziation by softirqs

After the tenth iteration(MAX_SOFTIRQ_RESTART) over the softirq bitmap, the kernel schedules the so-called ksoftirqd kernel thread, which takes control over the execution of softirqs.

Each processor has its own kernel thread called ksoftirqd/n, where n is the number of the processor

This processor-local kernel thread then executes softirqs as long as any bit in the softirq bitmap is set.

The aforementioned processor-monopolization is thus avoided by deferring softirq execution into process context (i.e., kernel thread), so that the ksoftirqd can be preempted by any other (user) process.

To check for the softirq threads use:
```
 ps -ef | grep ksoftirqd/
```

The spawn_ksoftirqd function starts these threads.

It is called early in the boot process.

```
static __init int spawn_ksoftirqd(void)
{
        cpuhp_setup_state_nocalls(CPUHP_SOFTIRQ_DEAD, "softirq:dead", NULL,
                                  takeover_tasklets);
        BUG_ON(smpboot_register_percpu_thread(&softirq_threads));

        return 0;
}
early_initcall(spawn_ksoftirqd);
```

File: kernel/softirq.c

Each ksoftirqd/n kernel thread runs the run_ksoftirqd()

```
static void run_ksoftirqd(unsigned int cpu)
{
        local_irq_disable();
        if (local_softirq_pending()) {
                /*
                 * We can safely run softirq on inline stack, as we are not deep
                 * in the task stack here.
                 */
                __do_softirq();
                local_irq_enable();
                cond_resched();
                return;
        }
        local_irq_enable();
}
```

If we want to check/test the ksoftirq, we can raise the softirq more that the max time:

```
void my_action(struct softirq_action *h)
{
	static int i = 0;
        pr_info("my_action\n");
	pr_info("current pid : %d , current process : %s\n",current->pid, current->comm);
	if (i++ <= 10)
		raise_softirq(MY_SOFTIRQ);
}
```

### Checking how many softirq are pending

It is 32-bit mask of pending softirqs. 

To check the number of pending irq use the function: `local_softirq_pending()` 

```

void my_action(struct softirq_action *h)
{
        pr_info("softirq started on processor id:%d\n", smp_processor_id());
	pr_info("local_softirq_pending:%02x\n", local_softirq_pending());
        pr_info("softirq ended on processor id:%d\n", smp_processor_id());
}

static irqreturn_t  button_handler(int irq, void *dev_id)
{
        pr_info("irq:%d, processor id:%d\n", irq, smp_processor_id());
	pr_info("local_softirq_pending:%02x\n", local_softirq_pending());
	raise_softirq(MY_SOFTIRQ);
	pr_info("local_softirq_pending:%02x\n", local_softirq_pending());
        return IRQ_HANDLED;
}
```

### When are pending softirqs run?

Pending softirq handlers are checked and executed at various points in the kernel code.

	a) After the completion of hard interrupt handlers with IRQ Lines Enabled
	   - do_IRQ() function finishes handling an I/O interrupt and invokes the `irq_exit()`

	b) Call to functions like `local_bh_enable()` or `spin_unlock_bh()`

	c) When one of the special ksoftirqd/n kernel threads is awakened

### in_softirq

You can tell you are in a softirq (or tasklet) using the `in_softirq()` macro.

### Disabling/Enabling Softirqs

If a softirq shares data with user context, you have two problems.
 - The current user context can be interrupted by a softirq
 - The critical region could be entered from another CPU

#### Solution to  first problem: the current user context can be interrupted by a softirq

```
void local_bh_disable() - Disable softirq and tasklet processing on the local processor

void local_bh_enable()	- Enable softirq and tasklet processing on the local processor
```
The calls can be nested only the final call to local_bh_enable() actually enables bottom halves.

#### Locking Between User Context and Softirqs

```
spin_lock_bh()	- Disables softirqs on the CPU and then grabs the lock

spin_unlock_bh() - Release lock and enable softirqs
```


## Tasklets

- reference: https://linux-kernel-labs.github.io/refs/heads/master/labs/deferred_work.html#tasklets
- reference: https://www.kernel.org/doc/html/v4.16/kernel-hacking/hacking.html#software-interrupt-context-softirqs-and-tasklets
- reference: https://embetronicx.com/tutorials/linux/device-drivers/tasklet-static-method/
- reference: https://elixir.bootlin.com/linux/v6.5.7/source/include/linux/interrupt.h#L642

Tasklets are bottom half mechanism built on top of softirqs.

Handlers of tasklets are executed by softirqs

#### Implementation


Tasklets are implemented on top of softirqs.

Tasklets are represented by two softirqs: HI_SOFTIRQ and TASKLET_SOFTIRQ.

The only difference in these types is that the HI_SOFTIRQ-based tasklets run prior to the TASKLET_SOFTIRQ based
tasklets.


### What are faster softirq or tasklets?

This depends on:

- https://elixir.bootlin.com/linux/v6.6.8/source/include/linux/interrupt.h#L548

```
enum
{
	HI_SOFTIRQ=0,
	TIMER_SOFTIRQ,
	NET_TX_SOFTIRQ,
	NET_RX_SOFTIRQ,
	BLOCK_SOFTIRQ,
	IRQ_POLL_SOFTIRQ,
	TASKLET_SOFTIRQ,
	SCHED_SOFTIRQ,
	HRTIMER_SOFTIRQ,
	RCU_SOFTIRQ,    /* Preferable RCU should always be the last softirq */

	NR_SOFTIRQS
};
```

Whatever is highier on that enum, has the highest priority. So HI_SOFTIRQ has the highest priority, then TASKLET_SOFTIRQ, and then SCHED_SOFTIRQ.


### Softirqs vs tasklets

```
			Softirqs			Tasklets	
		
Allocation:		Allocated at compile time	Can be dynamically registered

Reentrancy:		Yes, same softirqs can run	No, Same tasklet will not be scheduled
			on different processors		on different processors
```



### Can I sleep in tasklet handler?

As tasklets are based on softirqs, you cannot sleep.

You cannot use semaphores or other blocking functions in tasklet handler.



#### Data Structure


- reference: https://elixir.bootlin.com/linux/v6.5.7/source/include/linux/interrupt.h#L642

tasklet_struct @ Header File: <linux/interrupt.h>

```
/* Tasklets --- multithreaded analogue of BHs.

   This API is deprecated. Please consider using threaded IRQs instead:
   https://lore.kernel.org/lkml/20200716081538.2sivhkj4hcyrusem@linutronix.de

   Main feature differing them of generic softirqs: tasklet
   is running only on one CPU simultaneously.

   Main feature differing them of BHs: different tasklets
   may be run simultaneously on different CPUs.

   Properties:
   * If tasklet_schedule() is called, then tasklet is guaranteed
     to be executed on some cpu at least once after this.
   * If the tasklet is already scheduled, but its execution is still not
     started, it will be executed only once.
   * If this tasklet is already running on another CPU (or schedule is called
     from tasklet itself), it is rescheduled for later.
   * Tasklet is strictly serialized wrt itself, but not
     wrt another tasklets. If client needs some intertask synchronization,
     he makes it with spinlocks.
 */

struct tasklet_struct
{
	struct tasklet_struct *next;
	unsigned long state;
	atomic_t count;
	bool use_callback;
	union {
		void (*func)(unsigned long data);
		void (*callback)(struct tasklet_struct *t);
	};
	unsigned long data;
};

```

```
struct tasklet_struct
{
        struct tasklet_struct *next;     /* next tasklet in the list */
        unsigned long state;             /* state of the tasklet */
        atomic_t count;		         /* reference counter */
        void (*func)(unsigned long);     /* tasklet handler function */
        unsigned long data;              /* argument to the tasklet function */
};
```


#### state field

It can be 
 - a) 0
 - b) TASKLET_STATE_SCHED -  denotes a tasklet that is scheduled to run
 - c) TASKLET_STATE_RUN - denotes a tasklet that is running

TASKLET_STATE_RUN is used only on multiprocessor machines.
It is used to protect tasklets against concurrent execution on several processors.

#### count field

 - reference: https://elixir.bootlin.com/linux/v6.5.7/source/include/linux/interrupt.h#L686

```
enum
{
	TASKLET_STATE_SCHED,	/* Tasklet is scheduled for execution */
	TASKLET_STATE_RUN	/* Tasklet is running (SMP only) */
};
```

Used as a reference count for the tasklet
 - count = 0 - the tasklet is enabled and can run if marked pending.
 - count = nonzero - the tasklet is disabled and cannot run.


### Tasklet API

#### Declaring Tasklets


#### Static Initialization


DECLARE_TASKLET

```
	#define DECLARE_TASKLET(name, func, data) \
	struct tasklet_struct name = { NULL, 0, ATOMIC_INIT(0), func, data }
```

The first line:
```
	#define DECLARE_TASKLET(name, func, data) \
```

arguments:
- name: name of the tasklet
- func: function to execute
- data: arguments to the function to execute

The second line:
```
	struct tasklet_struct name = { NULL, 0, ATOMIC_INIT(0), func, data }
```

The arguments are: 
- list set to NULL, so no list.
- state set to 0, so TASKLET_STATE_SCHED, scheduled to run
- ATOMIC_INIT(0), counter to zero
- func: function
- data: arguments to the function


DECLARE_TASKLET_DISABLED

```
	#define DECLARE_TASKLET_DISABLED(name, func, data) \
	struct tasklet_struct name = { NULL, 0, ATOMIC_INIT(1), func, data }
```

Both these macros statically create a struct tasklet_struct with the given name.
When the tasklet is scheduled, the given function func is executed and passed data as argument.

The difference between the two macros is the initial reference count.

#### Dynamic tasklet definition

```
void tasklet_init(struct tasklet_struct *t,
                         void (*func)(unsigned long), unsigned long data);
```


### Example defining tasklets

```
#include <linux/kernel.h>
#include <linux/module.h>
#include <linux/interrupt.h>
#include <linux/delay.h>

MODULE_LICENSE("GPL");
char tasklet_data[] = "linux kernel is very easy";

void tasklet_function(unsigned long data)
{
	pr_info("%s:data:%s\n", __func__, (char *)data);
	return;
}

DECLARE_TASKLET(my_tasklet, tasklet_function, (unsigned long)&tasklet_data);
DECLARE_TASKLET_DISABLED(my_tasklet_disabled, tasklet_function, (unsigned long)&tasklet_data);

static int test_tasklet_init(void)
{
        pr_info("%s: In init\n", __func__);
	pr_info("State:%ld\n", my_tasklet.state);                       // zero value means enabled.
	pr_info("Count:%d\n", atomic_read(&my_tasklet.count));
	pr_info("State:%ld\n", my_tasklet_disabled.state);
	pr_info("Count:%d\n", atomic_read(&my_tasklet_disabled.count)); // non-zero vlaue means disabled.
	return 0;
}

static void test_tasklet_exit(void)
{
        pr_info("%s: In exit\n", __func__);
}

module_init(test_tasklet_init);
module_exit(test_tasklet_exit);
```

### Scheduling Tasklets


The kernel maintains two per-CPU tasklet linked lists for queuing scheduled tasklets

kernel/softirq.c : https://elixir.bootlin.com/linux/v6.5.7/source/kernel/softirq.c#L711

```
static DEFINE_PER_CPU(struct tasklet_head, tasklet_vec);
static DEFINE_PER_CPU(struct tasklet_head, tasklet_hi_vec);
```

 - tasklet_vec : for regular tasklets, run by TASKLET_SOFTIRQ and 
 - tasklet_hi_vec : for high-priority tasklets, run by HI_SOFTIRQ

Both of these structures are linked lists of tasklet_struct structures.

Each tasklet_struct structure in the list represents a different tasklet.

- https://elixir.bootlin.com/linux/v6.5.7/source/kernel/softirq.c#L711

```
struct tasklet_head {
        struct tasklet_struct *head;
        struct tasklet_struct **tail;
};
```

Tasklets are scheduled via the `tasklet_schedule()` and `tasklet_hi_schedule()`.

- https://elixir.bootlin.com/linux/v6.5.7/source/include/linux/interrupt.h#L709

```
static inline void tasklet_schedule(struct tasklet_struct *t)
{
        if (!test_and_set_bit(TASKLET_STATE_SCHED, &t->state))
                __tasklet_schedule(t);
}
```

The above function checks whether the tasklet is already scheduled, if not it atomically sets the state to
TASKLET_STATE_SCHED and invokes `__tasklet_schedule` to add the tasklet into the pending queue.

#### Steps performed by tasklet_schedule

1. Check whether the tasklet’s state is TASKLET_STATE_SCHED.
	If it is, the tasklet is already scheduled to run and the function can immediately `return`.

2. Call `__tasklet_schedule()`.

3. Save the state of the interrupt system, and then disable local interrupts by calling `local_irq_save`
	This ensures that nothing on this processor will mess with the tasklet code while `tasklet_schedule()` is manipulating the tasklets.

4. Add the tasklet to be scheduled to the head of the tasklet_vec or tasklet_hi_vec linked list, which is unique to each processor in the system.
	 
5. Raise the TASKLET_SOFTIRQ or HI_SOFTIRQ softirq, so `do_softirq()` executes this tasklet in the near future.

6. Restore interrupts to their previous state and return.


```
static void __tasklet_schedule_common(struct tasklet_struct *t,
                                      struct tasklet_head __percpu *headp,
                                      unsigned int softirq_nr)
{
        struct tasklet_head *head;
        unsigned long flags;

        local_irq_save(flags);       // disable local interrupts
        head = this_cpu_ptr(headp);
        t->next = NULL;
        *head->tail = t;
        head->tail = &(t->next);
        raise_softirq_irqoff(softirq_nr);  // raising softirq 
        local_irq_restore(flags);
}
```


### tasklet_hi_schedule

In addition to normal tasklets, the kernel uses a second kind of tasklet of a higher priority.

HI_SOFTIRQ is used as a softIRQ instead of TASKLET_SOFTIRQ

`tasklet_hi_schedule()` should be used if the tasklet should  run more urgently than networking, SCSI, timers 


#### Steps performed by tasklet softirq handlers


Handlers: `tasklet_action()`/`tasklet_hi_action()`

1. Disable local interrupt delivery and get the `tasklet_vec` or `tasklet_hi_vec` list for this processor

2. Clear the list for this processor by setting it equal to NULL.

3. Enable local interrupt delivery.
   
```
        struct tasklet_struct *list;

        local_irq_disable();
        list = tl_head->head;
        tl_head->head = NULL;
        tl_head->tail = &tl_head->head;
        local_irq_enable();
```

4. Loop over each pending tasklet in the retrieved list.

5. Check for a zero count value, to ensure the tasklet is not disabled. If the tasklet is disabled, skip it and go to the next pending tasklet.

6. Run the tasklet handler.

7. Repeat the next pending tasklet, until there are no more scheduled tasklets waiting to run.

```
        while (list) {
                struct tasklet_struct *t = list;

                list = list->next;

                if (tasklet_trylock(t)) {
                        if (!atomic_read(&t->count)) {
                                if (!test_and_clear_bit(TASKLET_STATE_SCHED,
                                                        &t->state))
                                        BUG();
                                t->func(t->data);
                                tasklet_unlock(t);
                                continue;
                        }
                        tasklet_unlock(t);
                }

                local_irq_disable();
                t->next = NULL;
                *tl_head->tail = t;
                tl_head->tail = &t->next;
                __raise_softirq_irqoff(softirq_nr);
                local_irq_enable();
        }
```


### Example scheduling a tasklet with static initialization

```
#include <linux/kernel.h>
#include <linux/module.h>
#include <linux/interrupt.h>
#include <linux/delay.h>

MODULE_LICENSE("GPL");

char tasklet_data[] = "linux kernel is very easy";
void tasklet_function(unsigned long data);

DECLARE_TASKLET(my_tasklet, tasklet_function, (unsigned long)&tasklet_data);

void tasklet_function(unsigned long data)
{
	pr_info("%s:data:%s\n", __func__, (char *)data);
	return;
}

static int test_tasklet_init(void)
{
        pr_info("%s: In init\n", __func__);
	pr_info("State:%ld\n", my_tasklet.state);
	pr_info("Count:%d\n", atomic_read(&my_tasklet.count));
	tasklet_schedule(&my_tasklet);	
	pr_info("State:%ld\n", my_tasklet.state);
	pr_info("Count:%d\n", atomic_read(&my_tasklet.count));
	return 0;
}

static void test_tasklet_exit(void)
{
        pr_info("%s: In exit\n", __func__);
}

module_init(test_tasklet_init);
module_exit(test_tasklet_exit);
```

### Example of scheduling a tasklet with dynamic initialization

```
#include <linux/kernel.h>
#include <linux/module.h>
#include <linux/interrupt.h>
#include <linux/slab.h>

MODULE_LICENSE("GPL");

char tasklet_data[] = "linux kernel is very easy";

void tasklet_function(unsigned long data)
{
	pr_info("%s:data:%s\n", __func__, (char *)data);
	return;
}

static struct tasklet_struct *my_tasklet;  // pointer to a tasklet

static int test_tasklet_init(void)
{
        pr_info("%s: In init\n", __func__);
	my_tasklet = kmalloc(sizeof(struct tasklet_struct), GFP_KERNEL); // allocate space for the tasklet
	pr_info("State:%ld\n", my_tasklet->state);
	pr_info("Count:%d\n", atomic_read(&my_tasklet->count));     // this is allocated with kmalloc, and this values will be garbage
	tasklet_init(my_tasklet, tasklet_function, tasklet_data);   // init the taskelt
	pr_info("State:%ld\n", my_tasklet->state);
	pr_info("Count:%d\n", atomic_read(&my_tasklet->count));
	tasklet_schedule(my_tasklet);	                            // schedule will change the state to 1
	pr_info("State:%ld\n", my_tasklet->state);
	pr_info("Count:%d\n", atomic_read(&my_tasklet->count));
	return 0;
}

static void test_tasklet_exit(void)
{
        pr_info("%s: In exit\n", __func__);
	kfree(my_tasklet);
}

module_init(test_tasklet_init);
module_exit(test_tasklet_exit);
```


### How kernel avoids running the same tasklet on multiple processors

With:
 - tasklet_trylock
 - tasklet_unlock

```
static inline int tasklet_trylock(struct tasklet_struct *t)
{
        return !test_and_set_bit(TASKLET_STATE_RUN, &(t)->state);
}
```

```
static inline void tasklet_unlock(struct tasklet_struct *t)
{
        smp_mb__before_atomic();
        clear_bit(TASKLET_STATE_RUN, &(t)->state);
}
```

On a multiprocessor machine, the kernel checks whether TASKLET_STATE_RUN is set (which means another processor is running this tasklet).

 - If set, do not execute now and skip to the next pending tasklet
 - Else set the TASKLET_STATE_RUN flag so that another processor cannot execute
 - After the tasklet completes, clear the TASKLET_STATE_RUN flag


### Can I sleep in tasklet handler?

As tasklets are based on softirqs, you cannot sleep.

You cannot use semaphores or other blocking functions in tasklet handler.


### Are interrupts enabled when tasklet runs?

```
#include <linux/kernel.h>
#include <linux/module.h>
#include <linux/interrupt.h>
#include <linux/slab.h>

MODULE_LICENSE("GPL");

char tasklet_data[] = "linux kernel is very easy";

void is_irq_disabled(void)
{
        if (irqs_disabled())
                pr_info("IRQ Disabled\n");
        else
                pr_info("IRQ Enabled\n");
}

void print_context(void)
{
        if (in_interrupt()) {
                pr_info("Code is running in interrupt context\n");
        } else {
                pr_info("Code is running in process context\n");
        }

        if (in_irq()) {
                pr_info("Code is running in hard irq context\n");
        } else {
                pr_info("Code is not running in hard irq context\n"); // a tasklet should not be running in an hard irq ocntext. ... because a tasklet is based on a softirq. 
        }

        if (in_softirq()) {
                pr_info("Code is running in soft irq context\n");  // a tasklet runs in a soft irq context
        } else {
                pr_info("Code is not running in soft irq context\n");
        }

}  

void tasklet_function(unsigned long data)
{
	pr_info("%s:data:%s\n", __func__, (char *)data);
        pr_info("2- local_softirq_pending:%02x\n", local_softirq_pending());
	is_irq_disabled();
        print_context();
	pr_info("current pid : %d , current process : %s\n",current->pid, current->comm); // in a tasklet this value can be any interrupted process.
        dump_stack();
	return;
}

static struct tasklet_struct *my_tasklet;

static int test_tasklet_init(void)
{
        pr_info("%s: In init\n", __func__);
	my_tasklet = kmalloc(sizeof(struct tasklet_struct), GFP_KERNEL);
	tasklet_init(my_tasklet, tasklet_function, tasklet_data);
        pr_info("1- local_softirq_pending:%02x\n", local_softirq_pending());
	// tasklet_hi_schedule(my_tasklet);
	tasklet_schedule(my_tasklet);
        pr_info("3- local_softirq_pending:%02x\n", local_softirq_pending());
	return 0;
}

static void test_tasklet_exit(void)
{
        pr_info("%s: In exit\n", __func__);
	kfree(my_tasklet);
}

module_init(test_tasklet_init);
module_exit(test_tasklet_exit);
```

Indeed IRQs are enabled.

### What is the context on a tasklet handler? 

A tasklet runs in an interrupt/atomic context. 

### What happens if i call tasklet_schedule twice?

After a tasklet is scheduled, it runs once at some time in the near future. If the same tasklet is scheduled again, before it has had a chance to run, it still runs only once


```
#include <linux/kernel.h>
#include <linux/module.h>
#include <linux/interrupt.h>
#include <linux/slab.h>
#include <linux/sched.h>


MODULE_LICENSE("GPL");

char tasklet_data[] = "linux kernel is very easy";

static struct tasklet_struct *my_tasklet;

const unsigned char kbdus[128] =
{
    0,  27, '1', '2', '3', '4', '5', '6', '7', '8',	/* 9 */
  '9', '0', '-', '=', '\b',	/* Backspace */
  '\t',			/* Tab */
  'q', 'w', 'e', 'r',	/* 19 */
  't', 'y', 'u', 'i', 'o', 'p', '[', ']', '\n',	/* Enter key */
    0,			/* 29   - Control */
  'a', 's', 'd', 'f', 'g', 'h', 'j', 'k', 'l', ';',	/* 39 */
 '\'', '`',   0,		/* Left shift */
 '\\', 'z', 'x', 'c', 'v', 'b', 'n',			/* 49 */
  'm', ',', '.', '/',   0,				/* Right shift */
  '*',
    0,	/* Alt */
  ' ',	/* Space bar */
    0,	/* Caps lock */
    0,	/* 59 - F1 key ... > */
    0,   0,   0,   0,   0,   0,   0,   0,
    0,	/* < ... F10 */
    0,	/* 69 - Num lock*/
    0,	/* Scroll Lock */
    0,	/* Home key */
    0,	/* Up Arrow */
    0,	/* Page Up */
  '-',
    0,	/* Left Arrow */
    0,
    0,	/* Right Arrow */
  '+',
    0,	/* 79 - End key*/
    0,	/* Down Arrow */
    0,	/* Page Down */
    0,	/* Insert Key */
    0,	/* Delete Key */
    0,   0,   0,
    0,	/* F11 Key */
    0,	/* F12 Key */
    0,	/* All other keys are undefined */
};

static int irq = 1,  dev = 0xaa;
#define KBD_DATA_REG        0x60    /* I/O port for keyboard data */
#define KBD_SCANCODE_MASK   0x7f
#define KBD_STATUS_MASK     0x80

static irqreturn_t keyboard_handler(int irq, void *dev)
{
        char scancode;
        scancode = inb(KBD_DATA_REG);
	pr_info("smp_processor_id:%d\n", smp_processor_id());
        pr_info("Character %c %s\n",
                        kbdus[scancode & KBD_SCANCODE_MASK],
                        scancode & KBD_STATUS_MASK ? "Released" : "Pressed");
	tasklet_schedule(my_tasklet);	
        return IRQ_NONE;
}

void tasklet_function(unsigned long data)
{
	pr_info("smp_processor_id:%d\n", smp_processor_id());
	pr_info("%s:data:%s\n", __func__, (char *)data);
	return;
}

static int test_tasklet_init(void)
{
        pr_info("%s: In init\n", __func__);
	my_tasklet = kmalloc(sizeof(struct tasklet_struct), GFP_KERNEL);
	tasklet_init(my_tasklet, tasklet_function, tasklet_data);
	return request_irq(irq, keyboard_handler, IRQF_SHARED,"my_keyboard_handler", &dev);
}

static void test_tasklet_exit(void)
{
        pr_info("%s: In exit\n", __func__);
	synchronize_irq(irq); /* synchronize interrupt */
        free_irq(irq, &dev);
	kfree(my_tasklet);
	
}

module_init(test_tasklet_init);
module_exit(test_tasklet_exit);
```

### Enabling/Disabling Tasklets


Disable tasklet
```
	tasklet_disable()
	tasklet_disable_nosync()
```
Enable tasklet

```
	tasklet_enable()
```

 - `tasklet_disable()` will wait if the tasklet is currently running and return only after it has finished execution
 - `tasklet_disable_nosync()` will not wait for the tasklet to complete prior to returning
```
static inline void tasklet_disable_nosync(struct tasklet_struct *t)
{
        atomic_inc(&t->count);
        smp_mb__after_atomic();
}
```

```
static inline void tasklet_disable(struct tasklet_struct *t)
{
        tasklet_disable_nosync(t);
        tasklet_unlock_wait(t);
        smp_mb();
}
```

```
static inline void tasklet_enable(struct tasklet_struct *t)
{
        smp_mb__before_atomic();
        atomic_dec(&t->count);
}
```

### tasklet_kill

```
void tasklet_kill(struct tasklet_struct *t);
```

The function removes a tasklet from the pending queue.

```
void tasklet_kill(struct tasklet_struct *t)
{
        if (in_interrupt())
                pr_notice("Attempt to kill tasklet from interrupt\n");

        while (test_and_set_bit(TASKLET_STATE_SCHED, &t->state)) {
                do {
                        yield();
                } while (test_bit(TASKLET_STATE_SCHED, &t->state));
        }
        tasklet_unlock_wait(t);
        clear_bit(TASKLET_STATE_SCHED, &t->state);
}
```


This function must not be used from interrupt context because it sleeps

If the tasklet specified is already scheduled by the time this call is invoked, then this function waits until its execution completes


### tasklet_hi_schedule

In addition to normal tasklets, the kernel uses a second kind of tasklet of a higher priority.

HI_SOFTIRQ is used as a softIRQ instead of TASKLET_SOFTIRQ

`tasklet_hi_schedule()` should be used if the tasklet should  run more urgently than networking, SCSI, timers 



## Work Queues


Introduced in Linux 2.6

They allow kernel functions to be activated and later executed by special kernel threads called worker threads.

Worker threads run in process context.

It is the only choice when you need to sleep in your bottom half (I/O data, hold mutexes/semaphores and all other functions that internally sleep)

Implementation: kernel/workqueue.c

### Design

work item: struct which hold the pointer to the function to be executed asynchronously

work queue: a queue of work items

Drivers add work item into the work queue

Worker Threads: Special purpose threads which execute the functions from the queue, one after the other

If no work is queued, the worker threads become idle,

```
ps -ef | grep kworker
```

Worker Pools:	A thread pool that is used to manage the worker threads

There are two worker-pools, 
 - one for normal work items and
 - the other for high priority ones,
 - some extra worker-pools to serve work items queued on unbound workqueues

`create_worker` is the function where kthreads are created



### Legacy Workqueues


Legacy workqueues have dedicated threads associated with them.

The new workqueues do away with that. There are no threads dedicated to any specific workqueue

Instead, there is a global pool of threads attached to each CPU in the system

When a work item is enqueued, it will be passed to one of the global threads at the right time


### How a target worker pool is determined when work item is queued into workqueue?

According to the queue parameters and workqueue attributes

### Data structures

 - workqueue 		--	struct workqueue_struct
 - work items		-- 	struct work_struct

```
struct work_struct {
        atomic_long_t data;
        struct list_head entry;
        work_func_t func;       // func is a pointer that takes the address of the deferred routine
};
```

 - func is a pointer that takes the address of the deferred routine

```
typedef void (*work_func_t)(struct work_struct *work);
```

### Initialization of Work Items

Header File: <linux/workqueue.h>

#### Static: 
declare and initialize a work item
```
DECLARE_WORK(name, void (*function)(void *), void *data);
```

#### Dynamic: 
initialize an already declared work item.
```
INIT_WORK(struct work_struct *work, void(*function)(struct work_struct *));
```

### API's to queue Work

This function enqueues the given work item on the local CPU workqueue, but does not guarantee its execution on it

```
bool queue_work(struct workqueue_struct *wq, struct work_struct *work);
```

Once queued, the function associated with the work item is executed on any of the **available** CPUs by the relevant
kworker thread.
	
### To queue work on a specific CPU

```
bool queue_work_on(int cpu, struct workqueue_struct *wq, struct work_struct *work)
```
 - cpu: CPU number to execute work on
 - Return: false if work was already on a queue, true otherwise.



### Workqueues

Header File: <linux/workqueue.h>

Workqueue API provides two types of function interfaces to
	a) Create own workqueue
	b) Use System Workqueue (`extern struct workqueue_struct *system_wq;`)
		

System workqueue is shared by all kernel subsystems and services


Example: 

```
#include <linux/kernel.h>
#include <linux/module.h>
#include <linux/interrupt.h>
#include <linux/delay.h>

MODULE_LICENSE("GPL");

struct work_struct work;
int cpu = 2;
module_param(cpu, int, 0);
static void work_fn(struct work_struct *work)
{
	pr_info("work_fn executed at processor id:%d\tdeferred work execution\n", smp_processor_id());
}

static int test_tasklet_init(void)
{
        pr_info("test_tasklet_init executed at processor id:%d: In init\n", smp_processor_id());
	INIT_WORK(&work, work_fn);
	if (queue_work_on(cpu, system_wq, &work))
		pr_info("work queued\n");
	else
		pr_err("work queuing failed\n");

	return 0;
}

static void test_tasklet_exit(void)
{
        pr_info("%s: In exit\n", __func__);
}

module_init(test_tasklet_init);
module_exit(test_tasklet_exit);
```

### Inline functions

 - schedule_work - put work task in global workqueue
 - schedule_work_on - put work task on a specific cpu

```
static inline bool schedule_work(struct work_struct *work)
{
        return queue_work(system_wq, work);
}
```
```
static inline bool schedule_work_on(int cpu, struct work_struct *work)
{
        return queue_work_on(cpu, system_wq, work);
}
```

Usually a work is enclosed in a larger structure (for driver private data)

```
#include <linux/kernel.h>
#include <linux/module.h>
#include <linux/interrupt.h>
#include <linux/delay.h>
#include <linux/sched.h>

MODULE_LICENSE("GPL");

struct work_struct work;
int cpu = 2;
module_param(cpu, int, 0);

typedef struct my_work{
        struct work_struct work;
        char data[20];
} my_work;

my_work deferred_work;

void print_context(void)
{
        if (in_interrupt()) {
                pr_info("Code is running in interrupt context\n");
        } else {
                pr_info("Code is running in process context\n");
        }
}

void is_irq_disabled(void)
{
        if (irqs_disabled())
                pr_info("IRQ Disabled\n");
        else
                pr_info("IRQ Enabled\n");
}


static void work_fn(struct work_struct *work)
{
	my_work *defer_work = (my_work *)container_of(work, my_work, work);
	is_irq_disabled();  // the irq should be enable so beware that you need to use proper locking mechanims.
	print_context();    // as this is a workqueue this should be running in process context.
	pr_info("processor id:%d\tdeferred work execution\n", smp_processor_id());
	pr_info("Data:%s\n", defer_work->data);
	pr_info("current pid : %d , current process : %s\n",current->pid, current->comm);
}

static int test_tasklet_init(void)
{
        pr_info("processor id:%d: In init\n", smp_processor_id());  
	pr_info("current pid : %d , current process : %s\n",current->pid, current->comm);
	print_context();
        is_irq_disabled();
	INIT_WORK(&deferred_work.work, work_fn);
	strcpy(deferred_work.data, "Linux is easy");
	if (schedule_work_on(cpu, &deferred_work.work))
		pr_info("work queued\n");
	else
		pr_err("work queuing failed\n");

	return 0;
}

static void test_tasklet_exit(void)
{
        pr_info("%s: In exit\n", __func__);
}

module_init(test_tasklet_init);
module_exit(test_tasklet_exit);
```


### Cancelling work


Work items can not be enabled/disabled but they can be canceled by calling cancel_work_sync()

bool cancel_work_sync(struct work_struct *work);

The call only stops the subsequent execution of the work item.

If the work item is already running at the time of the call, it will continue to run

Return:
 - true	- if work was pending
 - false - otherwise

### Scheduling more that one work items

```
#include <linux/kernel.h>
#include <linux/module.h>
#include <linux/interrupt.h>
#include <linux/delay.h>
#include <linux/sched.h>

MODULE_LICENSE("GPL");

struct work_struct work;
int cpu = 2;
module_param(cpu, int, 0);

typedef struct my_work{
        struct work_struct work;
        char data[20];
} my_work;

my_work deferred_work1, deferred_work2;

static void work_fn(struct work_struct *work)
{
	my_work *defer_work = (my_work *)container_of(work, my_work, work);
	pr_info("processor id:%d\tdeferred work execution\n", smp_processor_id());
	pr_info("Data:%s\n", defer_work->data);
	pr_info("current pid : %d , current process : %s\n",current->pid, current->comm);
}

static int test_tasklet_init(void)
{
        pr_info("processor id:%d: In init\n", smp_processor_id());
	INIT_WORK(&deferred_work1.work, work_fn);
	INIT_WORK(&deferred_work2.work, work_fn);
	strcpy(deferred_work1.data, "Linux is easy");
	strcpy(deferred_work2.data, "We make it easy");
	if (schedule_work_on(cpu, &deferred_work1.work))
		pr_info("work1 queued\n");
	else
		pr_err("work1 queuing failed\n");

	if (schedule_work_on(cpu, &deferred_work2.work))
		pr_info("work2 queued\n");
	else
		pr_err("work2 queuing failed\n");

	return 0;
}

static void test_tasklet_exit(void)
{
        pr_info("%s: In exit\n", __func__);
}

module_init(test_tasklet_init);
module_exit(test_tasklet_exit);
```

### Flushing work

```
bool flush_work(struct work_struct *work)
```
	
Wait for a work to finish executing the last queueing instance

returns true if waited for the work to finish execution, false if it was already idle


### Delayed work


Workqueue API allows you to queue work tasks whose execution is guaranteed to be delayed at least until a specified timeout.

This is achieved by binding a work task with a timer, which can be initialized with an expiry timeout, until which time the work task is not scheduled into the queue

```
struct delayed_work {
        struct work_struct work;
        struct timer_list timer;

        /* target workqueue and CPU ->timer uses to queue ->work */
        struct workqueue_struct *wq;
        int cpu;
};
```

timer is an instance of a dynamic timer descriptor, which is initialized with the expiry interval and armed while scheduling a work task

#### Initialization


#### Static:
```
DECLARE_DELAYED_WORK(name, void(*function)(struct work_struct *));
```
#### Dynamic:

```
INIT_DELAYED_WORK(struct delayed_work *work, void(*function)(struct work_struct *));
```

#### Scheduling

```
bool schedule_delayed_work(struct delayed_work *dwork,
                                         unsigned long delay);

bool schedule_delayed_work_on(int cpu, struct delayed_work *dwork,
                                            unsigned long delay);
```

delay needs to be provided in jiffies

```
#include <linux/kernel.h>
#include <linux/module.h>
#include <linux/interrupt.h>
#include <linux/delay.h>
#include <linux/jiffies.h>

MODULE_LICENSE("GPL");

struct delayed_work work;
int cpu = 2;
module_param(cpu, int, 0);

static void work_fn(struct work_struct *work)
{
	pr_info("processor id:%d\tdeferred work execution\n", smp_processor_id());
}

static int test_tasklet_init(void)
{
        pr_info("processor id:%d: In init\n", smp_processor_id());
	INIT_DELAYED_WORK(&work, work_fn);
	schedule_delayed_work_on(cpu, &work, msecs_to_jiffies(5000));
	return 0;
}

static void test_tasklet_exit(void)
{
        pr_info("%s: In exit\n", __func__);
}

module_init(test_tasklet_init);
module_exit(test_tasklet_exit);
```

for the delayed work there is a differentedelayed function:

```
bool flush_delayed_work(struct delayed_work *dwork);
```
delayed timer is cancelled and the pending work is queued for immediate execution

for cancelling the delayed work there is also other function:

```
bool cancel_delayed_work(struct delayed_work *dwork)
```


### Differences between Tasklets, softirqs and Workqueues

```
		------------------------------------------------------------------------------
		Softirqs			Tasklets			Workqueues
		-----------------------------------------------------------------------------
Execution	Interrupt context		Interrupt Context		Process Context
Context
----------------------------------------------------------------------------------------------------------------
Reentrancy	Yes(can run simultaneously	Cannot run same tasklets	Yes (can run simultaneously
		on different CPUs)		on different CPUs. Different	on different CPUs)
						CPUs can run different tasklets
--------------------------------------------------------------------------------------------------------------
Sleep		Cannot sleep			Cannot Sleep			Can Sleep
--------------------------------------------------------------------------------------------------------------
Preemption	Cannot be preempted/scheduled	Cannot be preempted/scheduled	May be preempted/scheduled
-----------------------------------------------------------------------------------------------------------
Ease of use	Not easy to use			Easy to use			Easy to use
-------------------------------------------------------------------------------------------------------------
When to use	If deferred work will not       If deferred work will not go    If deferred work needs to sleep
		go to sleep and have crucial	to sleep
		scalability or speed
		requirements
----------------------------------------------------------------------------------------------------------------
```


## kworker

kworker processes are kernel worker processes

From Documentation/kernel-per-CPU-kthreads.txt,

Naming convention: kworker/%u:%d%s (cpu, id, priority)

Worker threads are also two types:

 - CPU Bound  : It is named as kworker/<corenumber>:<id>
 - CPU Unbound: It is named as kworker/u<poolnumber>:<id>

```
$ ps -ef | grep 'kworker'
```
kworker/2:0H      -->   running on CPU2 and threadID 0 and is high priority bounded

The u designates a special CPU, the unbound cpu, meaning that the kthread is currently unbound
kworker/u257:0-   -->   unbound thread runs on any CPU

#### Use taskset to find the CPU Affinity

taskset is used to set or retrieve the CPU affinity of a running process given its PID or to launch a new COMMAND with a given CPU affinity.

```
$ taskset -p [PID]
```
Remember that the result is reported as a mask, that comes from hexadecimal, so 1 means the 1st CPU, 2 means snd CPU, 4 measn third CPU, 3 means CPUs one and two ...



#### To find out what any kworker is doing

```
$ cat /proc/$(pid_of_kworker)/stack
```

### Dedicated Workqueues

Timing of the execution of work items scheduled onto the global workqueue is not predictable.

one long-running work item can always cause indefinite delays for the rest

Alternatively, the workqueue framework allows the allocation of dedicated workqueues

```
struct workqueue_struct *alloc_workqueue(const char *fmt,
                                         unsigned int flags,
                                         int max_active, ...);
```

The above function allocates a workqueue.

Parameters:
 - fmt: printf format for the name of the workqueue
 - flags: control how work items are assigned execution resources, scheduled and executed.
 - max_active: This parameter limits the number of work items which can be executed simultaneously from this workqueue on any given CPU.
 - remaining args: args for @fmt

### destroy a workqueue

```
void destroy_workqueue(struct workqueue_struct *wq)
```

safely terminate a workqueue; all work currently pending will be done first.


### Workqueue flag: WQ_UNBOUND

Workqueues created with this flag are managed by kworker-pools that are not bound to any specific CPU.

Scheduled work items to this queue can run on any available processor.

Work items in this queue are executed as soon as possible by kworker pools.

Example:

```
#include <linux/kernel.h>
#include <linux/module.h>
#include <linux/interrupt.h>
#include <linux/delay.h>
#include <asm/current.h>
#include <linux/sched.h>

MODULE_LICENSE("GPL");

struct workqueue_struct *my_queue = NULL;
typedef struct my_work{
        struct work_struct work;
        char data[20];
}my_work;

my_work deferred_work;

static void work_fn(struct work_struct *work)
{
	my_work *defer_work = (my_work *)container_of(work, my_work, work);
	pr_info("deferred work execution\n");
	pr_info("Data:%s\n", defer_work->data);
	pr_info("current pid : %d , current process : %s\n",current->pid, current->comm);
	pr_info("processor id:%d\n", smp_processor_id());
}


static int test_tasklet_init(void)
{
        pr_info("%s: In init processorid:%d\n", __func__, smp_processor_id());
	INIT_WORK(&deferred_work.work, work_fn);
	strcpy(deferred_work.data, "Linux is easy");
	my_queue = alloc_workqueue("my_queue", WQ_UNBOUND, 1);
	queue_work(my_queue, &deferred_work.work);
	pr_info("%s: Init complete processor id:%d\n", __func__, smp_processor_id());
	return 0;
}

static void test_tasklet_exit(void)
{
        pr_info("%s: In exit\n", __func__);
	destroy_workqueue(my_queue);
}

module_init(test_tasklet_init);
module_exit(test_tasklet_exit);
```

### Queuing two workitems in workqueue with max active flags to 1


```
#include <linux/kernel.h>
#include <linux/module.h>
#include <linux/interrupt.h>
#include <linux/delay.h>
#include <asm/current.h>
#include <linux/sched.h>

MODULE_LICENSE("GPL");

struct workqueue_struct *my_queue = NULL;
typedef struct my_work{
        struct work_struct work;
        char data[20];
}my_work;

my_work deferred_work1, deferred_work2;

static void work_fn1(struct work_struct *work)
{
	my_work *defer_work = (my_work *)container_of(work, my_work, work);
	pr_info("starting deferred work1 execution on processor id:%d\n", smp_processor_id());
	pr_info("%s:Data:%s\n", __func__, defer_work->data);
	mdelay(4000);
	pr_info("%s:current pid : %d , current process : %s\n", __func__,current->pid, current->comm);
	mdelay(4000);
	pr_info("%s:processor id:%d\n", __func__, smp_processor_id());
	mdelay(4000);
	pr_info("ending deferred work 1 execution on processor:%d\n", smp_processor_id());
}

static void work_fn2(struct work_struct *work)
{
	//  the timings are to appreciate the execuition of this task.
	my_work *defer_work = (my_work *)container_of(work, my_work, work);
	pr_info("starting deferred work2 execution on processor id:%d\n", smp_processor_id());
	pr_info("%s:Data:%s\n", __func__, defer_work->data);
	mdelay(4000);
	pr_info("%s:current pid : %d , current process : %s\n", __func__,current->pid, current->comm);
	mdelay(4000);
	pr_info("%s:processor id:%d\n", __func__, smp_processor_id());
	mdelay(4000);
	pr_info("ending deferred work2 execution on processor:%d\n", smp_processor_id());
}

static int test_tasklet_init(void)
{
        pr_info("%s: In init processorid:%d\n", __func__, smp_processor_id());

	INIT_WORK(&deferred_work1.work, work_fn1);
	INIT_WORK(&deferred_work2.work, work_fn2);

	strcpy(deferred_work1.data, "Linux is easy");
	strcpy(deferred_work2.data, "Workqueues are easy");

	my_queue = alloc_workqueue("my_queue", WQ_UNBOUND, 1);

	queue_work(my_queue, &deferred_work1.work);
	queue_work(my_queue, &deferred_work2.work);
	pr_info("%s: Init complete processor id:%d\n", __func__, smp_processor_id());
	return 0;
}

static void test_tasklet_exit(void)
{
        pr_info("%s: In exit\n", __func__);
	destroy_workqueue(my_queue);
}

module_init(test_tasklet_init);
module_exit(test_tasklet_exit);
```

As the max active tasks is 1, only one task will be executed at a time. 

### Queuing two workitems in workqueue with max active flags to 2

```
#include <linux/kernel.h>
#include <linux/module.h>
#include <linux/interrupt.h>
#include <linux/delay.h>
#include <asm/current.h>
#include <linux/sched.h>

MODULE_LICENSE("GPL");

struct workqueue_struct *my_queue = NULL;
typedef struct my_work{
        struct work_struct work;
        char data[20];
}my_work;

my_work deferred_work1, deferred_work2;

static void work_fn1(struct work_struct *work)
{
	my_work *defer_work = (my_work *)container_of(work, my_work, work);
	pr_info("starting deferred work1 execution on processor id:%d\n", smp_processor_id());
	pr_info("%s:Data:%s\n", __func__, defer_work->data);
	mdelay(4000);
	pr_info("%s:current pid : %d , current process : %s\n", __func__,current->pid, current->comm);
	mdelay(4000);
	pr_info("%s:processor id:%d\n", __func__, smp_processor_id());
	mdelay(4000);
	pr_info("ending deferred work 1 execution on processor:%d\n", smp_processor_id());
}

static void work_fn2(struct work_struct *work)
{
	my_work *defer_work = (my_work *)container_of(work, my_work, work);
	pr_info("starting deferred work2 execution on processor id:%d\n", smp_processor_id());
	pr_info("%s:Data:%s\n", __func__, defer_work->data);
	mdelay(4000);
	pr_info("%s:current pid : %d , current process : %s\n", __func__,current->pid, current->comm);
	mdelay(4000);
	pr_info("%s:processor id:%d\n", __func__, smp_processor_id());
	mdelay(4000);
	pr_info("ending deferred work2 execution on processor:%d\n", smp_processor_id());
}

static int test_tasklet_init(void)
{
        pr_info("%s: In init processorid:%d\n", __func__, smp_processor_id());

	INIT_WORK(&deferred_work1.work, work_fn1);
	INIT_WORK(&deferred_work2.work, work_fn2);

	strcpy(deferred_work1.data, "Linux is easy");
	strcpy(deferred_work2.data, "Workqueues are easy");

	my_queue = alloc_workqueue("my_queue", WQ_UNBOUND, 2);

	queue_work(my_queue, &deferred_work1.work);
	queue_work(my_queue, &deferred_work2.work);
	pr_info("%s: Init complete processor id:%d\n", __func__, smp_processor_id());
	return 0;
}

static void test_tasklet_exit(void)
{
        pr_info("%s: In exit\n", __func__);
	destroy_workqueue(my_queue);
}

module_init(test_tasklet_init);
module_exit(test_tasklet_exit);
```

Now the tasks are executed in paralel.



### Workqueue flag: :WQ_HIGHPRI

This flag is used to mark a workqueue as high priority

Queued to the highpri worker-pool of the target cpu.

Highpri worker-pools are served by worker threads with elevated nice level.

#### Nice value: 

Nice value ranges from -20 (highest priority level) to 19 (lowest priority level)

Default value is 0.

To check the nice value:

```
$ ps ax -o pid,ni,cmd
```

The above command prints pid, nice value and process name

Example: 

```
#include <linux/kernel.h>
#include <linux/module.h>
#include <linux/interrupt.h>
#include <linux/delay.h>
#include <asm/current.h>
#include <linux/sched.h>

MODULE_LICENSE("GPL");

struct workqueue_struct *my_queue = NULL;
typedef struct my_work{
        struct work_struct work;
        char data[20];
}my_work;

my_work deferred_work;

static void work_fn(struct work_struct *work)
{
	my_work *defer_work = (my_work *)container_of(work, my_work, work);
	pr_info("deferred work execution\n");
	pr_info("Data:%s\n", defer_work->data);
	pr_info("current pid : %d , current process : %s\n",current->pid, current->comm);
	pr_info("processor id:%d\n", smp_processor_id());
}


static int test_tasklet_init(void)
{
        pr_info("%s: In init processorid:%d\n", __func__, smp_processor_id());
	INIT_WORK(&deferred_work.work, work_fn);
	strcpy(deferred_work.data, "Linux is easy");
	my_queue = alloc_workqueue("my_queue", WQ_HIGHPRI, 1);
	queue_work(my_queue, &deferred_work.work);
	pr_info("%s: Init complete processor id:%d\n", __func__, smp_processor_id());
	return 0;
}

static void test_tasklet_exit(void)
{
        pr_info("%s: In exit\n", __func__);
	destroy_workqueue(my_queue);
}

module_init(test_tasklet_init);
module_exit(test_tasklet_exit);
```

### Perform periodic task using workqueues

To perform some task periodically, requeue the work from the work function itself.

Example for performing a task periodically with workqueues:
```
#include <linux/kernel.h>
#include <linux/module.h>
#include <linux/interrupt.h>
#include <linux/delay.h>
#include <asm/current.h>
#include <linux/sched.h>
#include <linux/jiffies.h>

MODULE_LICENSE("GPL");

struct workqueue_struct *my_queue = NULL;
typedef struct my_work{
        struct delayed_work work;
        char data[20];
} my_work;

my_work deferred_work1, deferred_work2;

static void work_fn1(struct work_struct *work)
{
	struct delayed_work *my_delayed_work = to_delayed_work(work);
	my_work *defer_work = (my_work *)container_of(my_delayed_work, my_work, work);
	pr_info("starting deferred work1 execution on processor id:%d\n", smp_processor_id());
	pr_info("%s:Data:%s\n", __func__, defer_work->data);
	pr_info("%s:current pid : %d , current process : %s\n", __func__,current->pid, current->comm);
	pr_info("%s:processor id:%d\n", __func__, smp_processor_id());
	pr_info("ending deferred work 1 execution on processor:%d\n", smp_processor_id());
	queue_delayed_work(my_queue, &deferred_work1.work, msecs_to_jiffies(1000));
}

static void work_fn2(struct work_struct *work)
{
	struct delayed_work *my_delayed_work = to_delayed_work(work);
	my_work *defer_work = (my_work *)container_of(my_delayed_work, my_work, work);
	pr_info("starting deferred work2 execution on processor id:%d\n", smp_processor_id());
	pr_info("%s:Data:%s\n", __func__, defer_work->data);
	pr_info("%s:current pid : %d , current process : %s\n", __func__,current->pid, current->comm);
	pr_info("%s:processor id:%d\n", __func__, smp_processor_id());
	pr_info("ending deferred work2 execution on processor:%d\n", smp_processor_id());
	queue_delayed_work(my_queue, &deferred_work2.work, msecs_to_jiffies(1000));
}

static int test_tasklet_init(void)
{
        pr_info("%s: In init processorid:%d\n", __func__, smp_processor_id());

	INIT_DELAYED_WORK(&deferred_work1.work, work_fn1);
	INIT_DELAYED_WORK(&deferred_work2.work, work_fn2);

	strcpy(deferred_work1.data, "Linux is easy");
	strcpy(deferred_work2.data, "Workqueues are easy");

	my_queue = alloc_workqueue("my_queue", WQ_UNBOUND, 1);

	queue_delayed_work(my_queue, &deferred_work1.work, msecs_to_jiffies(1000));
	queue_delayed_work(my_queue, &deferred_work2.work, msecs_to_jiffies(1000));
	pr_info("%s: Init complete processor id:%d\n", __func__, smp_processor_id());
	return 0;
}

static void test_tasklet_exit(void)
{
        pr_info("%s: In exit\n", __func__);
	cancel_delayed_work(&deferred_work1.work);
	cancel_delayed_work(&deferred_work2.work);
	destroy_workqueue(my_queue);
}

module_init(test_tasklet_init);
module_exit(test_tasklet_exit);
```


### Workqueue flags: WQ_SYSFS

A given workqueue can be made visible in the sysfs filesystem by passing the WQ_SYSFS to that workqueue's `alloc_workqueue()`.

```
$ ls /sys/devices/virtual/workqueue

# echo 1 > /sys/devices/virtual/workqueue/cpumask
```

```
#include <linux/kernel.h>
#include <linux/module.h>
#include <linux/interrupt.h>
#include <linux/delay.h>
#include <asm/current.h>
#include <linux/sched.h>

MODULE_LICENSE("GPL");

struct workqueue_struct *my_queue = NULL;
typedef struct my_work{
        struct work_struct work;
        char data[20];
}my_work;

my_work deferred_work;

static void work_fn(struct work_struct *work)
{
	my_work *defer_work = (my_work *)container_of(work, my_work, work);
	pr_info("deferred work execution\n");
	pr_info("Data:%s\n", defer_work->data);
	pr_info("current pid : %d , current process : %s\n",current->pid, current->comm);
	pr_info("processor id:%d\n", smp_processor_id());
}

static int test_tasklet_init(void)
{
        pr_info("%s: In init processorid:%d\n", __func__, smp_processor_id());
	INIT_WORK(&deferred_work.work, work_fn);
	strcpy(deferred_work.data, "Linux is easy");
	my_queue = alloc_workqueue("my_queue", WQ_UNBOUND | WQ_SYSFS, 1);
	queue_work(my_queue, &deferred_work.work);
	pr_info("%s: Init complete processor id:%d\n", __func__, smp_processor_id());
	return 0;
}

static void test_tasklet_exit(void)
{
        pr_info("%s: In exit\n", __func__);
	destroy_workqueue(my_queue);
}

module_init(test_tasklet_init);
module_exit(test_tasklet_exit);
```

It is possible to change the cpu mask on sysfs to define in which cpus the workqueues are excuted: 

```
# cat /sys/devices/virutal/workqueue/cpumask # check the cpud masks.
```

```
#include <linux/kernel.h>
#include <linux/module.h>
#include <linux/interrupt.h>
#include <linux/delay.h>
#include <asm/current.h>
#include <linux/sched.h>
#include <linux/jiffies.h>

MODULE_LICENSE("GPL");

struct workqueue_struct *my_queue = NULL;
typedef struct my_work{
        struct delayed_work work;
        char data[20];
}my_work;

my_work deferred_work1;

static void work_fn1(struct work_struct *work)
{
	struct delayed_work *my_delayed_work = to_delayed_work(work);
	my_work *defer_work = (my_work *)container_of(my_delayed_work, my_work, work);
	pr_info("starting deferred work1 execution on processor id:%d\n", smp_processor_id());
	pr_info("%s:Data:%s\n", __func__, defer_work->data);
	pr_info("%s:current pid : %d , current process : %s\n", __func__,current->pid, current->comm);
	pr_info("%s:processor id:%d\n", __func__, smp_processor_id());
	pr_info("ending deferred work 1 execution on processor:%d\n", smp_processor_id());
	queue_delayed_work(my_queue, &deferred_work1.work, msecs_to_jiffies(1000));
}



static int test_tasklet_init(void)
{
        pr_info("%s: In init processorid:%d\n", __func__, smp_processor_id());

	INIT_DELAYED_WORK(&deferred_work1.work, work_fn1);

	strcpy(deferred_work1.data, "Linux is easy");

	my_queue = alloc_workqueue("my_queue", WQ_UNBOUND | WQ_SYSFS, 1);  // here is the name fo my_queue

	queue_delayed_work(my_queue, &deferred_work1.work, msecs_to_jiffies(1000));
	pr_info("%s: Init complete processor id:%d\n", __func__, smp_processor_id());
	return 0;
}

static void test_tasklet_exit(void)
{
        pr_info("%s: In exit\n", __func__);
	cancel_delayed_work(&deferred_work1.work);
	destroy_workqueue(my_queue);
}

module_init(test_tasklet_init);
module_exit(test_tasklet_exit);
```


```
# echo 2 > /sys/devices/virtual/workqueue/my_queue/cpumask
```

Note: cpumasks are maps, so the


### Other Flags


#### WQ_FREEZABLE:

A freezable wq participates in the freeze phase of the system  suspend operations.
This flag is used in the context of power management and file systems, and is especially important for creating the system image in the suspend phase
workqueues which can run tasks as part of the suspend/resume process should not have this flag set.
You can find more information about this topic in Documentation/power/freezing-of-tasks.txt.


#### WQ_MEM_RECLAIM:
	
All workqueues which might be used in the memory reclaim paths must have this flag set.
The workqueue is guaranteed to have at least one woker, a so-called rescuer thread, regardless of memory pressure.
Let us consider the following scenario:
 - Workqueue W has 3 items A, B and C.
     - A does some work and then waits until C has finished some work.
     - Afterwards, B does some GFP KERNEL allocations and blocks as there is not enough memory available.
     - As a result, C cannot run since B still occupies the W's worker;
     - another worker cannot be created because there is not enough memory.
     - A pre-allocated rescuer thread can solve this problem, by executing C which then wakes up A.
     - B will continue as soon as there is enough available memory to allocate.

#### WQ_CPU_INTENSIVE:

Tasks on this workqueue can be expected to use a fair amount of CPU time.

In other words, runnable CPU intensive work items will not prevent other work items in the same worker pool from starting execution.


### alloc_ordered_workqueue()

#define alloc_ordered_workqueue(fmt, flags, args...)

Allocate an ordered workqueue

An ordered workqueue executes at most one work item at any given time in the queued order

They are implemented as unbound workqueues with max_active of one
